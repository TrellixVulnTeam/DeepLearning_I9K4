{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsund\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting MNIST Dataset...\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n",
      "Data Extracted.\n"
     ]
    }
   ],
   "source": [
    "helper.download_dataset()\n",
    "train_images, train_labels, validation_images, validation_labels, test_images, test_labels = helper.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(shape=(None, 784), dtype=tf.float32, name='inputs')\n",
    "targets = tf.placeholder(shape=(None, 784), dtype=tf.float32, name='targets')\n",
    "\n",
    "#encoder\n",
    "hidden_layer1_size = 128\n",
    "hidden_layer1 = tf.layers.dense(inputs, hidden_layer1_size, activation=tf.nn.relu)\n",
    "\n",
    "hidden_layer2_size = 32\n",
    "encoded = tf.layers.dense(hidden_layer1, hidden_layer2_size, activation=tf.nn.relu)\n",
    "\n",
    "#decoder\n",
    "hidden_layer3_size = 128\n",
    "hidden_layer3 = tf.layers.dense(encoded, hidden_layer3_size, activation=tf.nn.relu)\n",
    "\n",
    "logits = tf.layers.dense(hidden_layer3, 784, activation=None)\n",
    "decoded = tf.nn.sigmoid(logits, name='outputs')\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits)\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "epochs = 30\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30.... Training loss : 0.693\n",
      "Epoch 1/30.... Training loss : 0.688\n",
      "Epoch 1/30.... Training loss : 0.682\n",
      "Epoch 1/30.... Training loss : 0.673\n",
      "Epoch 1/30.... Training loss : 0.658\n",
      "Epoch 1/30.... Training loss : 0.644\n",
      "Epoch 1/30.... Training loss : 0.631\n",
      "Epoch 1/30.... Training loss : 0.605\n",
      "Epoch 1/30.... Training loss : 0.584\n",
      "Epoch 1/30.... Training loss : 0.557\n",
      "Epoch 1/30.... Training loss : 0.517\n",
      "Epoch 1/30.... Training loss : 0.484\n",
      "Epoch 1/30.... Training loss : 0.450\n",
      "Epoch 1/30.... Training loss : 0.427\n",
      "Epoch 1/30.... Training loss : 0.395\n",
      "Epoch 1/30.... Training loss : 0.364\n",
      "Epoch 1/30.... Training loss : 0.349\n",
      "Epoch 1/30.... Training loss : 0.349\n",
      "Epoch 1/30.... Training loss : 0.339\n",
      "Epoch 1/30.... Training loss : 0.324\n",
      "Epoch 1/30.... Training loss : 0.380\n",
      "Epoch 1/30.... Training loss : 0.326\n",
      "Epoch 1/30.... Training loss : 0.292\n",
      "Epoch 1/30.... Training loss : 0.305\n",
      "Epoch 1/30.... Training loss : 0.291\n",
      "Epoch 1/30.... Training loss : 0.286\n",
      "Epoch 1/30.... Training loss : 0.275\n",
      "Epoch 1/30.... Training loss : 0.282\n",
      "Epoch 1/30.... Training loss : 0.295\n",
      "Epoch 1/30.... Training loss : 0.296\n",
      "Epoch 1/30.... Training loss : 0.287\n",
      "Epoch 1/30.... Training loss : 0.293\n",
      "Epoch 1/30.... Training loss : 0.288\n",
      "Epoch 1/30.... Training loss : 0.286\n",
      "Epoch 1/30.... Training loss : 0.277\n",
      "Epoch 1/30.... Training loss : 0.303\n",
      "Epoch 1/30.... Training loss : 0.284\n",
      "Epoch 1/30.... Training loss : 0.285\n",
      "Epoch 1/30.... Training loss : 0.280\n",
      "Epoch 1/30.... Training loss : 0.265\n",
      "Epoch 1/30.... Training loss : 0.259\n",
      "Epoch 1/30.... Training loss : 0.267\n",
      "Epoch 1/30.... Training loss : 0.264\n",
      "Epoch 1/30.... Training loss : 0.267\n",
      "Epoch 1/30.... Training loss : 0.253\n",
      "Epoch 1/30.... Training loss : 0.254\n",
      "Epoch 1/30.... Training loss : 0.260\n",
      "Epoch 1/30.... Training loss : 0.270\n",
      "Epoch 1/30.... Training loss : 0.262\n",
      "Epoch 1/30.... Training loss : 0.253\n",
      "Epoch 1/30.... Training loss : 0.244\n",
      "Epoch 1/30.... Training loss : 0.244\n",
      "Epoch 1/30.... Training loss : 0.238\n",
      "Epoch 1/30.... Training loss : 0.252\n",
      "Epoch 1/30.... Training loss : 0.248\n",
      "Epoch 1/30.... Training loss : 0.256\n",
      "Epoch 1/30.... Training loss : 0.266\n",
      "Epoch 1/30.... Training loss : 0.257\n",
      "Epoch 1/30.... Training loss : 0.279\n",
      "Epoch 1/30.... Training loss : 0.253\n",
      "Epoch 1/30.... Training loss : 0.244\n",
      "Epoch 1/30.... Training loss : 0.239\n",
      "Epoch 1/30.... Training loss : 0.246\n",
      "Epoch 1/30.... Training loss : 0.249\n",
      "Epoch 1/30.... Training loss : 0.244\n",
      "Epoch 1/30.... Training loss : 0.240\n",
      "Epoch 1/30.... Training loss : 0.255\n",
      "Epoch 1/30.... Training loss : 0.241\n",
      "Epoch 1/30.... Training loss : 0.247\n",
      "Epoch 1/30.... Training loss : 0.251\n",
      "Epoch 1/30.... Training loss : 0.254\n",
      "Epoch 1/30.... Training loss : 0.234\n",
      "Epoch 1/30.... Training loss : 0.234\n",
      "Epoch 1/30.... Training loss : 0.249\n",
      "Epoch 1/30.... Training loss : 0.213\n",
      "Epoch 1/30.... Training loss : 0.236\n",
      "Epoch 1/30.... Training loss : 0.237\n",
      "Epoch 1/30.... Training loss : 0.215\n",
      "Epoch 1/30.... Training loss : 0.227\n",
      "Epoch 1/30.... Training loss : 0.235\n",
      "Epoch 1/30.... Training loss : 0.228\n",
      "Epoch 1/30.... Training loss : 0.211\n",
      "Epoch 1/30.... Training loss : 0.216\n",
      "Epoch 1/30.... Training loss : 0.214\n",
      "Epoch 1/30.... Training loss : 0.220\n",
      "Epoch 1/30.... Training loss : 0.208\n",
      "Epoch 1/30.... Training loss : 0.230\n",
      "Epoch 1/30.... Training loss : 0.230\n",
      "Epoch 1/30.... Training loss : 0.215\n",
      "Epoch 1/30.... Training loss : 0.214\n",
      "Epoch 1/30.... Training loss : 0.222\n",
      "Epoch 1/30.... Training loss : 0.225\n",
      "Epoch 1/30.... Training loss : 0.220\n",
      "Epoch 1/30.... Training loss : 0.235\n",
      "Epoch 1/30.... Training loss : 0.221\n",
      "Epoch 1/30.... Training loss : 0.207\n",
      "Epoch 1/30.... Training loss : 0.204\n",
      "Epoch 1/30.... Training loss : 0.221\n",
      "Epoch 1/30.... Training loss : 0.216\n",
      "Epoch 1/30.... Training loss : 0.214\n",
      "Epoch 1/30.... Training loss : 0.205\n",
      "Epoch 1/30.... Training loss : 0.200\n",
      "Epoch 1/30.... Training loss : 0.207\n",
      "Epoch 1/30.... Training loss : 0.204\n",
      "Epoch 1/30.... Training loss : 0.211\n",
      "Epoch 1/30.... Training loss : 0.207\n",
      "Epoch 1/30.... Training loss : 0.217\n",
      "Epoch 1/30.... Training loss : 0.217\n",
      "Epoch 1/30.... Training loss : 0.195\n",
      "Epoch 1/30.... Training loss : 0.208\n",
      "Epoch 1/30.... Training loss : 0.206\n",
      "Epoch 1/30.... Training loss : 0.199\n",
      "Epoch 1/30.... Training loss : 0.185\n",
      "Epoch 1/30.... Training loss : 0.197\n",
      "Epoch 1/30.... Training loss : 0.197\n",
      "Epoch 1/30.... Training loss : 0.199\n",
      "Epoch 1/30.... Training loss : 0.200\n",
      "Epoch 1/30.... Training loss : 0.196\n",
      "Epoch 1/30.... Training loss : 0.199\n",
      "Epoch 1/30.... Training loss : 0.198\n",
      "Epoch 1/30.... Training loss : 0.197\n",
      "Epoch 1/30.... Training loss : 0.206\n",
      "Epoch 1/30.... Training loss : 0.202\n",
      "Epoch 1/30.... Training loss : 0.205\n",
      "Epoch 1/30.... Training loss : 0.194\n",
      "Epoch 1/30.... Training loss : 0.189\n",
      "Epoch 1/30.... Training loss : 0.199\n",
      "Epoch 1/30.... Training loss : 0.201\n",
      "Epoch 1/30.... Training loss : 0.196\n",
      "Epoch 1/30.... Training loss : 0.192\n",
      "Epoch 1/30.... Training loss : 0.186\n",
      "Epoch 1/30.... Training loss : 0.195\n",
      "Epoch 1/30.... Training loss : 0.188\n",
      "Epoch 1/30.... Training loss : 0.186\n",
      "Epoch 1/30.... Training loss : 0.188\n",
      "Epoch 1/30.... Training loss : 0.190\n",
      "Epoch 1/30.... Training loss : 0.189\n",
      "Epoch 1/30.... Training loss : 0.186\n",
      "Epoch 1/30.... Training loss : 0.197\n",
      "Epoch 1/30.... Training loss : 0.186\n",
      "Epoch 1/30.... Training loss : 0.181\n",
      "Epoch 1/30.... Training loss : 0.194\n",
      "Epoch 1/30.... Training loss : 0.196\n",
      "Epoch 1/30.... Training loss : 0.183\n",
      "Epoch 1/30.... Training loss : 0.187\n",
      "Epoch 1/30.... Training loss : 0.189\n",
      "Epoch 1/30.... Training loss : 0.191\n",
      "Epoch 1/30.... Training loss : 0.190\n",
      "Epoch 1/30.... Training loss : 0.173\n",
      "Epoch 1/30.... Training loss : 0.179\n",
      "Epoch 1/30.... Training loss : 0.183\n",
      "Epoch 1/30.... Training loss : 0.179\n",
      "Epoch 1/30.... Training loss : 0.186\n",
      "Epoch 1/30.... Training loss : 0.190\n",
      "Epoch 1/30.... Training loss : 0.180\n",
      "Epoch 1/30.... Training loss : 0.185\n",
      "Epoch 1/30.... Training loss : 0.182\n",
      "Epoch 1/30.... Training loss : 0.185\n",
      "Epoch 1/30.... Training loss : 0.183\n",
      "Epoch 1/30.... Training loss : 0.187\n",
      "Epoch 1/30.... Training loss : 0.182\n",
      "Epoch 1/30.... Training loss : 0.188\n",
      "Epoch 1/30.... Training loss : 0.185\n",
      "Epoch 1/30.... Training loss : 0.182\n",
      "Epoch 1/30.... Training loss : 0.177\n",
      "Epoch 1/30.... Training loss : 0.183\n",
      "Epoch 1/30.... Training loss : 0.194\n",
      "Epoch 1/30.... Training loss : 0.174\n",
      "Epoch 1/30.... Training loss : 0.177\n",
      "Epoch 1/30.... Training loss : 0.183\n",
      "Epoch 1/30.... Training loss : 0.185\n",
      "Epoch 1/30.... Training loss : 0.177\n",
      "Epoch 1/30.... Training loss : 0.194\n",
      "Epoch 1/30.... Training loss : 0.185\n",
      "Epoch 1/30.... Training loss : 0.185\n",
      "Epoch 1/30.... Training loss : 0.179\n",
      "Epoch 1/30.... Training loss : 0.173\n",
      "Epoch 1/30.... Training loss : 0.179\n",
      "Epoch 1/30.... Training loss : 0.177\n",
      "Epoch 1/30.... Training loss : 0.166\n",
      "Epoch 1/30.... Training loss : 0.179\n",
      "Epoch 1/30.... Training loss : 0.178\n",
      "Epoch 1/30.... Training loss : 0.179\n",
      "Epoch 1/30.... Training loss : 0.177\n",
      "Epoch 1/30.... Training loss : 0.171\n",
      "Epoch 1/30.... Training loss : 0.169\n",
      "Epoch 1/30.... Training loss : 0.169\n",
      "Epoch 1/30.... Training loss : 0.179\n",
      "Epoch 1/30.... Training loss : 0.168\n",
      "Epoch 1/30.... Training loss : 0.167\n",
      "Epoch 1/30.... Training loss : 0.172\n",
      "Epoch 1/30.... Training loss : 0.180\n",
      "Epoch 1/30.... Training loss : 0.168\n",
      "Epoch 1/30.... Training loss : 0.167\n",
      "Epoch 1/30.... Training loss : 0.168\n",
      "Epoch 1/30.... Training loss : 0.168\n",
      "Epoch 1/30.... Training loss : 0.162\n",
      "Epoch 1/30.... Training loss : 0.173\n",
      "Epoch 1/30.... Training loss : 0.167\n",
      "Epoch 1/30.... Training loss : 0.167\n",
      "Epoch 1/30.... Training loss : 0.166\n",
      "Epoch 1/30.... Training loss : 0.176\n",
      "Epoch 1/30.... Training loss : 0.165\n",
      "Epoch 1/30.... Training loss : 0.164\n",
      "Epoch 1/30.... Training loss : 0.168\n",
      "Epoch 1/30.... Training loss : 0.171\n",
      "Epoch 1/30.... Training loss : 0.169\n",
      "Epoch 1/30.... Training loss : 0.169\n",
      "Epoch 1/30.... Training loss : 0.171\n",
      "Epoch 1/30.... Training loss : 0.154\n",
      "Epoch 1/30.... Training loss : 0.173\n",
      "Epoch 1/30.... Training loss : 0.156\n",
      "Epoch 1/30.... Training loss : 0.207\n",
      "Epoch 1/30.... Training loss : 0.165\n",
      "Epoch 1/30.... Training loss : 0.165\n",
      "Epoch 2/30.... Training loss : 0.168\n",
      "Epoch 2/30.... Training loss : 0.170\n",
      "Epoch 2/30.... Training loss : 0.166\n",
      "Epoch 2/30.... Training loss : 0.163\n",
      "Epoch 2/30.... Training loss : 0.164\n",
      "Epoch 2/30.... Training loss : 0.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30.... Training loss : 0.161\n",
      "Epoch 2/30.... Training loss : 0.167\n",
      "Epoch 2/30.... Training loss : 0.170\n",
      "Epoch 2/30.... Training loss : 0.162\n",
      "Epoch 2/30.... Training loss : 0.166\n",
      "Epoch 2/30.... Training loss : 0.174\n",
      "Epoch 2/30.... Training loss : 0.162\n",
      "Epoch 2/30.... Training loss : 0.168\n",
      "Epoch 2/30.... Training loss : 0.170\n",
      "Epoch 2/30.... Training loss : 0.163\n",
      "Epoch 2/30.... Training loss : 0.167\n",
      "Epoch 2/30.... Training loss : 0.174\n",
      "Epoch 2/30.... Training loss : 0.168\n",
      "Epoch 2/30.... Training loss : 0.160\n",
      "Epoch 2/30.... Training loss : 0.177\n",
      "Epoch 2/30.... Training loss : 0.162\n",
      "Epoch 2/30.... Training loss : 0.158\n",
      "Epoch 2/30.... Training loss : 0.155\n",
      "Epoch 2/30.... Training loss : 0.153\n",
      "Epoch 2/30.... Training loss : 0.156\n",
      "Epoch 2/30.... Training loss : 0.151\n",
      "Epoch 2/30.... Training loss : 0.153\n",
      "Epoch 2/30.... Training loss : 0.166\n",
      "Epoch 2/30.... Training loss : 0.170\n",
      "Epoch 2/30.... Training loss : 0.164\n",
      "Epoch 2/30.... Training loss : 0.166\n",
      "Epoch 2/30.... Training loss : 0.164\n",
      "Epoch 2/30.... Training loss : 0.158\n",
      "Epoch 2/30.... Training loss : 0.163\n",
      "Epoch 2/30.... Training loss : 0.173\n",
      "Epoch 2/30.... Training loss : 0.164\n",
      "Epoch 2/30.... Training loss : 0.170\n",
      "Epoch 2/30.... Training loss : 0.163\n",
      "Epoch 2/30.... Training loss : 0.158\n",
      "Epoch 2/30.... Training loss : 0.156\n",
      "Epoch 2/30.... Training loss : 0.162\n",
      "Epoch 2/30.... Training loss : 0.163\n",
      "Epoch 2/30.... Training loss : 0.160\n",
      "Epoch 2/30.... Training loss : 0.150\n",
      "Epoch 2/30.... Training loss : 0.158\n",
      "Epoch 2/30.... Training loss : 0.163\n",
      "Epoch 2/30.... Training loss : 0.166\n",
      "Epoch 2/30.... Training loss : 0.164\n",
      "Epoch 2/30.... Training loss : 0.164\n",
      "Epoch 2/30.... Training loss : 0.149\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.151\n",
      "Epoch 2/30.... Training loss : 0.164\n",
      "Epoch 2/30.... Training loss : 0.153\n",
      "Epoch 2/30.... Training loss : 0.160\n",
      "Epoch 2/30.... Training loss : 0.161\n",
      "Epoch 2/30.... Training loss : 0.152\n",
      "Epoch 2/30.... Training loss : 0.171\n",
      "Epoch 2/30.... Training loss : 0.159\n",
      "Epoch 2/30.... Training loss : 0.150\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.155\n",
      "Epoch 2/30.... Training loss : 0.160\n",
      "Epoch 2/30.... Training loss : 0.156\n",
      "Epoch 2/30.... Training loss : 0.154\n",
      "Epoch 2/30.... Training loss : 0.165\n",
      "Epoch 2/30.... Training loss : 0.157\n",
      "Epoch 2/30.... Training loss : 0.157\n",
      "Epoch 2/30.... Training loss : 0.157\n",
      "Epoch 2/30.... Training loss : 0.159\n",
      "Epoch 2/30.... Training loss : 0.150\n",
      "Epoch 2/30.... Training loss : 0.155\n",
      "Epoch 2/30.... Training loss : 0.166\n",
      "Epoch 2/30.... Training loss : 0.143\n",
      "Epoch 2/30.... Training loss : 0.153\n",
      "Epoch 2/30.... Training loss : 0.156\n",
      "Epoch 2/30.... Training loss : 0.144\n",
      "Epoch 2/30.... Training loss : 0.153\n",
      "Epoch 2/30.... Training loss : 0.150\n",
      "Epoch 2/30.... Training loss : 0.151\n",
      "Epoch 2/30.... Training loss : 0.150\n",
      "Epoch 2/30.... Training loss : 0.145\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.154\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.164\n",
      "Epoch 2/30.... Training loss : 0.160\n",
      "Epoch 2/30.... Training loss : 0.151\n",
      "Epoch 2/30.... Training loss : 0.149\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.150\n",
      "Epoch 2/30.... Training loss : 0.150\n",
      "Epoch 2/30.... Training loss : 0.162\n",
      "Epoch 2/30.... Training loss : 0.156\n",
      "Epoch 2/30.... Training loss : 0.144\n",
      "Epoch 2/30.... Training loss : 0.144\n",
      "Epoch 2/30.... Training loss : 0.155\n",
      "Epoch 2/30.... Training loss : 0.149\n",
      "Epoch 2/30.... Training loss : 0.153\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.149\n",
      "Epoch 2/30.... Training loss : 0.151\n",
      "Epoch 2/30.... Training loss : 0.155\n",
      "Epoch 2/30.... Training loss : 0.151\n",
      "Epoch 2/30.... Training loss : 0.151\n",
      "Epoch 2/30.... Training loss : 0.151\n",
      "Epoch 2/30.... Training loss : 0.147\n",
      "Epoch 2/30.... Training loss : 0.149\n",
      "Epoch 2/30.... Training loss : 0.152\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.137\n",
      "Epoch 2/30.... Training loss : 0.144\n",
      "Epoch 2/30.... Training loss : 0.147\n",
      "Epoch 2/30.... Training loss : 0.149\n",
      "Epoch 2/30.... Training loss : 0.149\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.147\n",
      "Epoch 2/30.... Training loss : 0.147\n",
      "Epoch 2/30.... Training loss : 0.149\n",
      "Epoch 2/30.... Training loss : 0.155\n",
      "Epoch 2/30.... Training loss : 0.154\n",
      "Epoch 2/30.... Training loss : 0.152\n",
      "Epoch 2/30.... Training loss : 0.147\n",
      "Epoch 2/30.... Training loss : 0.141\n",
      "Epoch 2/30.... Training loss : 0.158\n",
      "Epoch 2/30.... Training loss : 0.155\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.149\n",
      "Epoch 2/30.... Training loss : 0.145\n",
      "Epoch 2/30.... Training loss : 0.153\n",
      "Epoch 2/30.... Training loss : 0.145\n",
      "Epoch 2/30.... Training loss : 0.137\n",
      "Epoch 2/30.... Training loss : 0.141\n",
      "Epoch 2/30.... Training loss : 0.152\n",
      "Epoch 2/30.... Training loss : 0.151\n",
      "Epoch 2/30.... Training loss : 0.146\n",
      "Epoch 2/30.... Training loss : 0.150\n",
      "Epoch 2/30.... Training loss : 0.145\n",
      "Epoch 2/30.... Training loss : 0.143\n",
      "Epoch 2/30.... Training loss : 0.151\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.142\n",
      "Epoch 2/30.... Training loss : 0.140\n",
      "Epoch 2/30.... Training loss : 0.141\n",
      "Epoch 2/30.... Training loss : 0.146\n",
      "Epoch 2/30.... Training loss : 0.145\n",
      "Epoch 2/30.... Training loss : 0.138\n",
      "Epoch 2/30.... Training loss : 0.137\n",
      "Epoch 2/30.... Training loss : 0.143\n",
      "Epoch 2/30.... Training loss : 0.140\n",
      "Epoch 2/30.... Training loss : 0.143\n",
      "Epoch 2/30.... Training loss : 0.147\n",
      "Epoch 2/30.... Training loss : 0.138\n",
      "Epoch 2/30.... Training loss : 0.146\n",
      "Epoch 2/30.... Training loss : 0.140\n",
      "Epoch 2/30.... Training loss : 0.144\n",
      "Epoch 2/30.... Training loss : 0.144\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.144\n",
      "Epoch 2/30.... Training loss : 0.148\n",
      "Epoch 2/30.... Training loss : 0.146\n",
      "Epoch 2/30.... Training loss : 0.142\n",
      "Epoch 2/30.... Training loss : 0.139\n",
      "Epoch 2/30.... Training loss : 0.145\n",
      "Epoch 2/30.... Training loss : 0.149\n",
      "Epoch 2/30.... Training loss : 0.136\n",
      "Epoch 2/30.... Training loss : 0.136\n",
      "Epoch 2/30.... Training loss : 0.142\n",
      "Epoch 2/30.... Training loss : 0.145\n",
      "Epoch 2/30.... Training loss : 0.138\n",
      "Epoch 2/30.... Training loss : 0.151\n",
      "Epoch 2/30.... Training loss : 0.144\n",
      "Epoch 2/30.... Training loss : 0.145\n",
      "Epoch 2/30.... Training loss : 0.142\n",
      "Epoch 2/30.... Training loss : 0.136\n",
      "Epoch 2/30.... Training loss : 0.142\n",
      "Epoch 2/30.... Training loss : 0.140\n",
      "Epoch 2/30.... Training loss : 0.128\n",
      "Epoch 2/30.... Training loss : 0.137\n",
      "Epoch 2/30.... Training loss : 0.141\n",
      "Epoch 2/30.... Training loss : 0.141\n",
      "Epoch 2/30.... Training loss : 0.140\n",
      "Epoch 2/30.... Training loss : 0.136\n",
      "Epoch 2/30.... Training loss : 0.135\n",
      "Epoch 2/30.... Training loss : 0.135\n",
      "Epoch 2/30.... Training loss : 0.142\n",
      "Epoch 2/30.... Training loss : 0.135\n",
      "Epoch 2/30.... Training loss : 0.136\n",
      "Epoch 2/30.... Training loss : 0.139\n",
      "Epoch 2/30.... Training loss : 0.144\n",
      "Epoch 2/30.... Training loss : 0.136\n",
      "Epoch 2/30.... Training loss : 0.135\n",
      "Epoch 2/30.... Training loss : 0.132\n",
      "Epoch 2/30.... Training loss : 0.137\n",
      "Epoch 2/30.... Training loss : 0.133\n",
      "Epoch 2/30.... Training loss : 0.137\n",
      "Epoch 2/30.... Training loss : 0.134\n",
      "Epoch 2/30.... Training loss : 0.134\n",
      "Epoch 2/30.... Training loss : 0.132\n",
      "Epoch 2/30.... Training loss : 0.141\n",
      "Epoch 2/30.... Training loss : 0.132\n",
      "Epoch 2/30.... Training loss : 0.132\n",
      "Epoch 2/30.... Training loss : 0.137\n",
      "Epoch 2/30.... Training loss : 0.139\n",
      "Epoch 2/30.... Training loss : 0.136\n",
      "Epoch 2/30.... Training loss : 0.138\n",
      "Epoch 2/30.... Training loss : 0.139\n",
      "Epoch 2/30.... Training loss : 0.126\n",
      "Epoch 2/30.... Training loss : 0.145\n",
      "Epoch 2/30.... Training loss : 0.128\n",
      "Epoch 2/30.... Training loss : 0.145\n",
      "Epoch 2/30.... Training loss : 0.136\n",
      "Epoch 2/30.... Training loss : 0.130\n",
      "Epoch 3/30.... Training loss : 0.135\n",
      "Epoch 3/30.... Training loss : 0.135\n",
      "Epoch 3/30.... Training loss : 0.133\n",
      "Epoch 3/30.... Training loss : 0.129\n",
      "Epoch 3/30.... Training loss : 0.128\n",
      "Epoch 3/30.... Training loss : 0.131\n",
      "Epoch 3/30.... Training loss : 0.128\n",
      "Epoch 3/30.... Training loss : 0.134\n",
      "Epoch 3/30.... Training loss : 0.135\n",
      "Epoch 3/30.... Training loss : 0.132\n",
      "Epoch 3/30.... Training loss : 0.136\n",
      "Epoch 3/30.... Training loss : 0.137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30.... Training loss : 0.132\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.132\n",
      "Epoch 3/30.... Training loss : 0.129\n",
      "Epoch 3/30.... Training loss : 0.132\n",
      "Epoch 3/30.... Training loss : 0.141\n",
      "Epoch 3/30.... Training loss : 0.139\n",
      "Epoch 3/30.... Training loss : 0.130\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.132\n",
      "Epoch 3/30.... Training loss : 0.131\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.134\n",
      "Epoch 3/30.... Training loss : 0.141\n",
      "Epoch 3/30.... Training loss : 0.135\n",
      "Epoch 3/30.... Training loss : 0.134\n",
      "Epoch 3/30.... Training loss : 0.133\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.135\n",
      "Epoch 3/30.... Training loss : 0.137\n",
      "Epoch 3/30.... Training loss : 0.132\n",
      "Epoch 3/30.... Training loss : 0.140\n",
      "Epoch 3/30.... Training loss : 0.133\n",
      "Epoch 3/30.... Training loss : 0.130\n",
      "Epoch 3/30.... Training loss : 0.128\n",
      "Epoch 3/30.... Training loss : 0.135\n",
      "Epoch 3/30.... Training loss : 0.131\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.132\n",
      "Epoch 3/30.... Training loss : 0.134\n",
      "Epoch 3/30.... Training loss : 0.136\n",
      "Epoch 3/30.... Training loss : 0.136\n",
      "Epoch 3/30.... Training loss : 0.136\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.120\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.137\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.131\n",
      "Epoch 3/30.... Training loss : 0.130\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.141\n",
      "Epoch 3/30.... Training loss : 0.134\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.130\n",
      "Epoch 3/30.... Training loss : 0.132\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.136\n",
      "Epoch 3/30.... Training loss : 0.133\n",
      "Epoch 3/30.... Training loss : 0.128\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.130\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.130\n",
      "Epoch 3/30.... Training loss : 0.139\n",
      "Epoch 3/30.... Training loss : 0.118\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.131\n",
      "Epoch 3/30.... Training loss : 0.119\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.121\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.119\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.137\n",
      "Epoch 3/30.... Training loss : 0.134\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.121\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.135\n",
      "Epoch 3/30.... Training loss : 0.130\n",
      "Epoch 3/30.... Training loss : 0.120\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.130\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.128\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.131\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.128\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.113\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.121\n",
      "Epoch 3/30.... Training loss : 0.114\n",
      "Epoch 3/30.... Training loss : 0.131\n",
      "Epoch 3/30.... Training loss : 0.129\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.128\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.114\n",
      "Epoch 3/30.... Training loss : 0.118\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.121\n",
      "Epoch 3/30.... Training loss : 0.130\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.120\n",
      "Epoch 3/30.... Training loss : 0.120\n",
      "Epoch 3/30.... Training loss : 0.118\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.119\n",
      "Epoch 3/30.... Training loss : 0.120\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.120\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.128\n",
      "Epoch 3/30.... Training loss : 0.119\n",
      "Epoch 3/30.... Training loss : 0.128\n",
      "Epoch 3/30.... Training loss : 0.121\n",
      "Epoch 3/30.... Training loss : 0.125\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.130\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.119\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.118\n",
      "Epoch 3/30.... Training loss : 0.116\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.119\n",
      "Epoch 3/30.... Training loss : 0.131\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.120\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.112\n",
      "Epoch 3/30.... Training loss : 0.116\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.121\n",
      "Epoch 3/30.... Training loss : 0.120\n",
      "Epoch 3/30.... Training loss : 0.118\n",
      "Epoch 3/30.... Training loss : 0.118\n",
      "Epoch 3/30.... Training loss : 0.126\n",
      "Epoch 3/30.... Training loss : 0.119\n",
      "Epoch 3/30.... Training loss : 0.118\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.128\n",
      "Epoch 3/30.... Training loss : 0.117\n",
      "Epoch 3/30.... Training loss : 0.116\n",
      "Epoch 3/30.... Training loss : 0.116\n",
      "Epoch 3/30.... Training loss : 0.119\n",
      "Epoch 3/30.... Training loss : 0.115\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.117\n",
      "Epoch 3/30.... Training loss : 0.119\n",
      "Epoch 3/30.... Training loss : 0.117\n",
      "Epoch 3/30.... Training loss : 0.123\n",
      "Epoch 3/30.... Training loss : 0.115\n",
      "Epoch 3/30.... Training loss : 0.116\n",
      "Epoch 3/30.... Training loss : 0.120\n",
      "Epoch 3/30.... Training loss : 0.124\n",
      "Epoch 3/30.... Training loss : 0.121\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.121\n",
      "Epoch 3/30.... Training loss : 0.112\n",
      "Epoch 3/30.... Training loss : 0.127\n",
      "Epoch 3/30.... Training loss : 0.114\n",
      "Epoch 3/30.... Training loss : 0.122\n",
      "Epoch 3/30.... Training loss : 0.120\n",
      "Epoch 3/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.120\n",
      "Epoch 4/30.... Training loss : 0.119\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.119\n",
      "Epoch 4/30.... Training loss : 0.120\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.120\n",
      "Epoch 4/30.... Training loss : 0.119\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30.... Training loss : 0.121\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.110\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.110\n",
      "Epoch 4/30.... Training loss : 0.110\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.108\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.118\n",
      "Epoch 4/30.... Training loss : 0.124\n",
      "Epoch 4/30.... Training loss : 0.120\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.120\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.120\n",
      "Epoch 4/30.... Training loss : 0.121\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.125\n",
      "Epoch 4/30.... Training loss : 0.119\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.121\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.121\n",
      "Epoch 4/30.... Training loss : 0.123\n",
      "Epoch 4/30.... Training loss : 0.121\n",
      "Epoch 4/30.... Training loss : 0.124\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.106\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.123\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.123\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.119\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.122\n",
      "Epoch 4/30.... Training loss : 0.120\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.125\n",
      "Epoch 4/30.... Training loss : 0.107\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.118\n",
      "Epoch 4/30.... Training loss : 0.108\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.107\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.110\n",
      "Epoch 4/30.... Training loss : 0.125\n",
      "Epoch 4/30.... Training loss : 0.122\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.120\n",
      "Epoch 4/30.... Training loss : 0.121\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.121\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.118\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.119\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.118\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.103\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.104\n",
      "Epoch 4/30.... Training loss : 0.119\n",
      "Epoch 4/30.... Training loss : 0.118\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.118\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.106\n",
      "Epoch 4/30.... Training loss : 0.108\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.120\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.110\n",
      "Epoch 4/30.... Training loss : 0.107\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.118\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.118\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.119\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.118\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.108\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.110\n",
      "Epoch 4/30.... Training loss : 0.120\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.110\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.104\n",
      "Epoch 4/30.... Training loss : 0.107\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.113\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.107\n",
      "Epoch 4/30.... Training loss : 0.110\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.108\n",
      "Epoch 4/30.... Training loss : 0.110\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.117\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.107\n",
      "Epoch 4/30.... Training loss : 0.107\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.106\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.109\n",
      "Epoch 4/30.... Training loss : 0.108\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.105\n",
      "Epoch 4/30.... Training loss : 0.108\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.116\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.114\n",
      "Epoch 4/30.... Training loss : 0.112\n",
      "Epoch 4/30.... Training loss : 0.105\n",
      "Epoch 4/30.... Training loss : 0.120\n",
      "Epoch 4/30.... Training loss : 0.107\n",
      "Epoch 4/30.... Training loss : 0.115\n",
      "Epoch 4/30.... Training loss : 0.111\n",
      "Epoch 4/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.111\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.103\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.115\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.103\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30.... Training loss : 0.103\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.116\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.117\n",
      "Epoch 5/30.... Training loss : 0.111\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.115\n",
      "Epoch 5/30.... Training loss : 0.114\n",
      "Epoch 5/30.... Training loss : 0.116\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.100\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.116\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.115\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.103\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.115\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.118\n",
      "Epoch 5/30.... Training loss : 0.100\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.111\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.103\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.101\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.118\n",
      "Epoch 5/30.... Training loss : 0.114\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.103\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.114\n",
      "Epoch 5/30.... Training loss : 0.115\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.111\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.097\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.099\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.111\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.099\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.111\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.111\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.111\n",
      "Epoch 5/30.... Training loss : 0.111\n",
      "Epoch 5/30.... Training loss : 0.103\n",
      "Epoch 5/30.... Training loss : 0.101\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.114\n",
      "Epoch 5/30.... Training loss : 0.111\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.099\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.108\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.103\n",
      "Epoch 5/30.... Training loss : 0.105\n",
      "Epoch 5/30.... Training loss : 0.110\n",
      "Epoch 5/30.... Training loss : 0.112\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.101\n",
      "Epoch 5/30.... Training loss : 0.106\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.103\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.100\n",
      "Epoch 5/30.... Training loss : 0.104\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.111\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.100\n",
      "Epoch 5/30.... Training loss : 0.113\n",
      "Epoch 5/30.... Training loss : 0.102\n",
      "Epoch 5/30.... Training loss : 0.109\n",
      "Epoch 5/30.... Training loss : 0.107\n",
      "Epoch 5/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.098\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.110\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.098\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.098\n",
      "Epoch 6/30.... Training loss : 0.098\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.112\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.109\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.100\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.109\n",
      "Epoch 6/30.... Training loss : 0.110\n",
      "Epoch 6/30.... Training loss : 0.109\n",
      "Epoch 6/30.... Training loss : 0.111\n",
      "Epoch 6/30.... Training loss : 0.098\n",
      "Epoch 6/30.... Training loss : 0.096\n",
      "Epoch 6/30.... Training loss : 0.100\n",
      "Epoch 6/30.... Training loss : 0.112\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.100\n",
      "Epoch 6/30.... Training loss : 0.110\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.100\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.110\n",
      "Epoch 6/30.... Training loss : 0.109\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.100\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.113\n",
      "Epoch 6/30.... Training loss : 0.097\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.098\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.097\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.100\n",
      "Epoch 6/30.... Training loss : 0.114\n",
      "Epoch 6/30.... Training loss : 0.110\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.110\n",
      "Epoch 6/30.... Training loss : 0.111\n",
      "Epoch 6/30.... Training loss : 0.098\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.110\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.095\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.096\n",
      "Epoch 6/30.... Training loss : 0.109\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.096\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.109\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.100\n",
      "Epoch 6/30.... Training loss : 0.098\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.109\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.100\n",
      "Epoch 6/30.... Training loss : 0.098\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.110\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.096\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.102\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.100\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.108\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.098\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.100\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.100\n",
      "Epoch 6/30.... Training loss : 0.106\n",
      "Epoch 6/30.... Training loss : 0.097\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.105\n",
      "Epoch 6/30.... Training loss : 0.104\n",
      "Epoch 6/30.... Training loss : 0.098\n",
      "Epoch 6/30.... Training loss : 0.110\n",
      "Epoch 6/30.... Training loss : 0.099\n",
      "Epoch 6/30.... Training loss : 0.107\n",
      "Epoch 6/30.... Training loss : 0.103\n",
      "Epoch 6/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.095\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.107\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.097\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.095\n",
      "Epoch 7/30.... Training loss : 0.095\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.107\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.109\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.097\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.108\n",
      "Epoch 7/30.... Training loss : 0.095\n",
      "Epoch 7/30.... Training loss : 0.093\n",
      "Epoch 7/30.... Training loss : 0.097\n",
      "Epoch 7/30.... Training loss : 0.108\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.097\n",
      "Epoch 7/30.... Training loss : 0.107\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.097\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.107\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.097\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.110\n",
      "Epoch 7/30.... Training loss : 0.094\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.095\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.095\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.097\n",
      "Epoch 7/30.... Training loss : 0.110\n",
      "Epoch 7/30.... Training loss : 0.107\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.107\n",
      "Epoch 7/30.... Training loss : 0.108\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.092\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.094\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.093\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.097\n",
      "Epoch 7/30.... Training loss : 0.095\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.108\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.093\n",
      "Epoch 7/30.... Training loss : 0.097\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.104\n",
      "Epoch 7/30.... Training loss : 0.106\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.095\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.098\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.094\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 7/30.... Training loss : 0.102\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.103\n",
      "Epoch 7/30.... Training loss : 0.101\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.107\n",
      "Epoch 7/30.... Training loss : 0.096\n",
      "Epoch 7/30.... Training loss : 0.105\n",
      "Epoch 7/30.... Training loss : 0.100\n",
      "Epoch 7/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.093\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.104\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.095\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.093\n",
      "Epoch 8/30.... Training loss : 0.093\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.104\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.106\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.095\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.104\n",
      "Epoch 8/30.... Training loss : 0.104\n",
      "Epoch 8/30.... Training loss : 0.105\n",
      "Epoch 8/30.... Training loss : 0.093\n",
      "Epoch 8/30.... Training loss : 0.091\n",
      "Epoch 8/30.... Training loss : 0.095\n",
      "Epoch 8/30.... Training loss : 0.106\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.095\n",
      "Epoch 8/30.... Training loss : 0.104\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.095\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.104\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.095\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.108\n",
      "Epoch 8/30.... Training loss : 0.092\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.093\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.093\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.093\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.095\n",
      "Epoch 8/30.... Training loss : 0.108\n",
      "Epoch 8/30.... Training loss : 0.104\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.104\n",
      "Epoch 8/30.... Training loss : 0.105\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.104\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.090\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.092\n",
      "Epoch 8/30.... Training loss : 0.104\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.091\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.104\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.095\n",
      "Epoch 8/30.... Training loss : 0.093\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.105\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.091\n",
      "Epoch 8/30.... Training loss : 0.095\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.095\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.102\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.096\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.092\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 8/30.... Training loss : 0.100\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.101\n",
      "Epoch 8/30.... Training loss : 0.099\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.105\n",
      "Epoch 8/30.... Training loss : 0.094\n",
      "Epoch 8/30.... Training loss : 0.103\n",
      "Epoch 8/30.... Training loss : 0.098\n",
      "Epoch 8/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.091\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.102\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.092\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.092\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.091\n",
      "Epoch 9/30.... Training loss : 0.091\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.102\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.104\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.092\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30.... Training loss : 0.102\n",
      "Epoch 9/30.... Training loss : 0.103\n",
      "Epoch 9/30.... Training loss : 0.091\n",
      "Epoch 9/30.... Training loss : 0.089\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.104\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.092\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.102\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.106\n",
      "Epoch 9/30.... Training loss : 0.090\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.091\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.091\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.091\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.106\n",
      "Epoch 9/30.... Training loss : 0.102\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.102\n",
      "Epoch 9/30.... Training loss : 0.104\n",
      "Epoch 9/30.... Training loss : 0.092\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.089\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.090\n",
      "Epoch 9/30.... Training loss : 0.102\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.090\n",
      "Epoch 9/30.... Training loss : 0.092\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.102\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.092\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.102\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.102\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.102\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.091\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.103\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.090\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.097\n",
      "Epoch 9/30.... Training loss : 0.092\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.094\n",
      "Epoch 9/30.... Training loss : 0.100\n",
      "Epoch 9/30.... Training loss : 0.090\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.099\n",
      "Epoch 9/30.... Training loss : 0.098\n",
      "Epoch 9/30.... Training loss : 0.092\n",
      "Epoch 9/30.... Training loss : 0.104\n",
      "Epoch 9/30.... Training loss : 0.093\n",
      "Epoch 9/30.... Training loss : 0.101\n",
      "Epoch 9/30.... Training loss : 0.096\n",
      "Epoch 9/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.090\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.090\n",
      "Epoch 10/30.... Training loss : 0.090\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.102\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.090\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.101\n",
      "Epoch 10/30.... Training loss : 0.089\n",
      "Epoch 10/30.... Training loss : 0.087\n",
      "Epoch 10/30.... Training loss : 0.091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30.... Training loss : 0.102\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.104\n",
      "Epoch 10/30.... Training loss : 0.088\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.089\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.090\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.090\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.104\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.102\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.088\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.089\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.088\n",
      "Epoch 10/30.... Training loss : 0.090\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.090\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.102\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.089\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.093\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.089\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 10/30.... Training loss : 0.097\n",
      "Epoch 10/30.... Training loss : 0.100\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.098\n",
      "Epoch 10/30.... Training loss : 0.096\n",
      "Epoch 10/30.... Training loss : 0.091\n",
      "Epoch 10/30.... Training loss : 0.102\n",
      "Epoch 10/30.... Training loss : 0.092\n",
      "Epoch 10/30.... Training loss : 0.099\n",
      "Epoch 10/30.... Training loss : 0.095\n",
      "Epoch 10/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.088\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.089\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.089\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.088\n",
      "Epoch 11/30.... Training loss : 0.088\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.100\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.089\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.099\n",
      "Epoch 11/30.... Training loss : 0.100\n",
      "Epoch 11/30.... Training loss : 0.088\n",
      "Epoch 11/30.... Training loss : 0.086\n",
      "Epoch 11/30.... Training loss : 0.090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30.... Training loss : 0.101\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.102\n",
      "Epoch 11/30.... Training loss : 0.087\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.088\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.089\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.089\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.102\n",
      "Epoch 11/30.... Training loss : 0.099\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.099\n",
      "Epoch 11/30.... Training loss : 0.101\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.087\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.088\n",
      "Epoch 11/30.... Training loss : 0.099\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.087\n",
      "Epoch 11/30.... Training loss : 0.089\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.099\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.089\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.099\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.099\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.099\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.099\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.089\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.101\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.099\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.088\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.096\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.099\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 11/30.... Training loss : 0.091\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.088\n",
      "Epoch 11/30.... Training loss : 0.092\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.097\n",
      "Epoch 11/30.... Training loss : 0.095\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.101\n",
      "Epoch 11/30.... Training loss : 0.090\n",
      "Epoch 11/30.... Training loss : 0.098\n",
      "Epoch 11/30.... Training loss : 0.094\n",
      "Epoch 11/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.087\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.088\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.088\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.087\n",
      "Epoch 12/30.... Training loss : 0.087\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.099\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.088\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.098\n",
      "Epoch 12/30.... Training loss : 0.099\n",
      "Epoch 12/30.... Training loss : 0.087\n",
      "Epoch 12/30.... Training loss : 0.085\n",
      "Epoch 12/30.... Training loss : 0.089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30.... Training loss : 0.100\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.101\n",
      "Epoch 12/30.... Training loss : 0.086\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.087\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.088\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.088\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.101\n",
      "Epoch 12/30.... Training loss : 0.098\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.098\n",
      "Epoch 12/30.... Training loss : 0.099\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.086\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.087\n",
      "Epoch 12/30.... Training loss : 0.098\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.086\n",
      "Epoch 12/30.... Training loss : 0.088\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.098\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.088\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.098\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.098\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.098\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.098\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.088\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.099\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.098\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.087\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.095\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.087\n",
      "Epoch 12/30.... Training loss : 0.091\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.096\n",
      "Epoch 12/30.... Training loss : 0.094\n",
      "Epoch 12/30.... Training loss : 0.089\n",
      "Epoch 12/30.... Training loss : 0.100\n",
      "Epoch 12/30.... Training loss : 0.090\n",
      "Epoch 12/30.... Training loss : 0.097\n",
      "Epoch 12/30.... Training loss : 0.093\n",
      "Epoch 12/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.086\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.087\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.087\n",
      "Epoch 13/30.... Training loss : 0.088\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.086\n",
      "Epoch 13/30.... Training loss : 0.086\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.098\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.087\n",
      "Epoch 13/30.... Training loss : 0.088\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.097\n",
      "Epoch 13/30.... Training loss : 0.098\n",
      "Epoch 13/30.... Training loss : 0.086\n",
      "Epoch 13/30.... Training loss : 0.084\n",
      "Epoch 13/30.... Training loss : 0.088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30.... Training loss : 0.099\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.088\n",
      "Epoch 13/30.... Training loss : 0.088\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.088\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.100\n",
      "Epoch 13/30.... Training loss : 0.086\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.086\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.087\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.087\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.100\n",
      "Epoch 13/30.... Training loss : 0.097\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.097\n",
      "Epoch 13/30.... Training loss : 0.098\n",
      "Epoch 13/30.... Training loss : 0.088\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.085\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.086\n",
      "Epoch 13/30.... Training loss : 0.097\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.085\n",
      "Epoch 13/30.... Training loss : 0.087\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.097\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.087\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.097\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.097\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.097\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.097\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.087\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.098\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.097\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.086\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.094\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.088\n",
      "Epoch 13/30.... Training loss : 0.088\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.088\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.090\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.086\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.095\n",
      "Epoch 13/30.... Training loss : 0.093\n",
      "Epoch 13/30.... Training loss : 0.088\n",
      "Epoch 13/30.... Training loss : 0.099\n",
      "Epoch 13/30.... Training loss : 0.089\n",
      "Epoch 13/30.... Training loss : 0.096\n",
      "Epoch 13/30.... Training loss : 0.092\n",
      "Epoch 13/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.086\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.086\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.086\n",
      "Epoch 14/30.... Training loss : 0.087\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.086\n",
      "Epoch 14/30.... Training loss : 0.086\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.097\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.086\n",
      "Epoch 14/30.... Training loss : 0.087\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.096\n",
      "Epoch 14/30.... Training loss : 0.097\n",
      "Epoch 14/30.... Training loss : 0.085\n",
      "Epoch 14/30.... Training loss : 0.084\n",
      "Epoch 14/30.... Training loss : 0.087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30.... Training loss : 0.098\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.087\n",
      "Epoch 14/30.... Training loss : 0.087\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.099\n",
      "Epoch 14/30.... Training loss : 0.085\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.085\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.086\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.086\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.099\n",
      "Epoch 14/30.... Training loss : 0.096\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.096\n",
      "Epoch 14/30.... Training loss : 0.097\n",
      "Epoch 14/30.... Training loss : 0.087\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.084\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.085\n",
      "Epoch 14/30.... Training loss : 0.096\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.084\n",
      "Epoch 14/30.... Training loss : 0.087\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.096\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.086\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.096\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.096\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.096\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.096\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.086\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.098\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.096\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.086\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.096\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.087\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.087\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.089\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.085\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.092\n",
      "Epoch 14/30.... Training loss : 0.094\n",
      "Epoch 14/30.... Training loss : 0.093\n",
      "Epoch 14/30.... Training loss : 0.087\n",
      "Epoch 14/30.... Training loss : 0.098\n",
      "Epoch 14/30.... Training loss : 0.088\n",
      "Epoch 14/30.... Training loss : 0.095\n",
      "Epoch 14/30.... Training loss : 0.091\n",
      "Epoch 14/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.085\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.086\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.086\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.085\n",
      "Epoch 15/30.... Training loss : 0.085\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.096\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.085\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.096\n",
      "Epoch 15/30.... Training loss : 0.084\n",
      "Epoch 15/30.... Training loss : 0.083\n",
      "Epoch 15/30.... Training loss : 0.086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30.... Training loss : 0.097\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.086\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.098\n",
      "Epoch 15/30.... Training loss : 0.084\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.084\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.086\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.086\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.099\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.097\n",
      "Epoch 15/30.... Training loss : 0.086\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.084\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.085\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.084\n",
      "Epoch 15/30.... Training loss : 0.086\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.086\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.085\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.097\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.091\n",
      "Epoch 15/30.... Training loss : 0.085\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.088\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.093\n",
      "Epoch 15/30.... Training loss : 0.085\n",
      "Epoch 15/30.... Training loss : 0.089\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.095\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.092\n",
      "Epoch 15/30.... Training loss : 0.086\n",
      "Epoch 15/30.... Training loss : 0.097\n",
      "Epoch 15/30.... Training loss : 0.087\n",
      "Epoch 15/30.... Training loss : 0.094\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 15/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.085\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.085\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.085\n",
      "Epoch 16/30.... Training loss : 0.086\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.084\n",
      "Epoch 16/30.... Training loss : 0.084\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.095\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.085\n",
      "Epoch 16/30.... Training loss : 0.086\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.095\n",
      "Epoch 16/30.... Training loss : 0.095\n",
      "Epoch 16/30.... Training loss : 0.084\n",
      "Epoch 16/30.... Training loss : 0.082\n",
      "Epoch 16/30.... Training loss : 0.086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30.... Training loss : 0.096\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.086\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.086\n",
      "Epoch 16/30.... Training loss : 0.086\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.086\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.097\n",
      "Epoch 16/30.... Training loss : 0.084\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.084\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.085\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.085\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.098\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.095\n",
      "Epoch 16/30.... Training loss : 0.096\n",
      "Epoch 16/30.... Training loss : 0.086\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.083\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.084\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.083\n",
      "Epoch 16/30.... Training loss : 0.085\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.085\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.095\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.095\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.085\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.096\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.084\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.086\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.086\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.088\n",
      "Epoch 16/30.... Training loss : 0.092\n",
      "Epoch 16/30.... Training loss : 0.084\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.093\n",
      "Epoch 16/30.... Training loss : 0.091\n",
      "Epoch 16/30.... Training loss : 0.086\n",
      "Epoch 16/30.... Training loss : 0.097\n",
      "Epoch 16/30.... Training loss : 0.087\n",
      "Epoch 16/30.... Training loss : 0.094\n",
      "Epoch 16/30.... Training loss : 0.090\n",
      "Epoch 16/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.084\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.085\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.084\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.084\n",
      "Epoch 17/30.... Training loss : 0.084\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.094\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.084\n",
      "Epoch 17/30.... Training loss : 0.085\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.094\n",
      "Epoch 17/30.... Training loss : 0.095\n",
      "Epoch 17/30.... Training loss : 0.083\n",
      "Epoch 17/30.... Training loss : 0.082\n",
      "Epoch 17/30.... Training loss : 0.085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30.... Training loss : 0.096\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.085\n",
      "Epoch 17/30.... Training loss : 0.085\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.097\n",
      "Epoch 17/30.... Training loss : 0.083\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.083\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.085\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.085\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.097\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.094\n",
      "Epoch 17/30.... Training loss : 0.095\n",
      "Epoch 17/30.... Training loss : 0.085\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.083\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.084\n",
      "Epoch 17/30.... Training loss : 0.094\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.083\n",
      "Epoch 17/30.... Training loss : 0.085\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.094\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.084\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.094\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.094\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.094\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.094\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.084\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.096\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.094\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.090\n",
      "Epoch 17/30.... Training loss : 0.084\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.094\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.085\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.087\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.084\n",
      "Epoch 17/30.... Training loss : 0.088\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.092\n",
      "Epoch 17/30.... Training loss : 0.091\n",
      "Epoch 17/30.... Training loss : 0.085\n",
      "Epoch 17/30.... Training loss : 0.096\n",
      "Epoch 17/30.... Training loss : 0.086\n",
      "Epoch 17/30.... Training loss : 0.093\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 17/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.084\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.084\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.084\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.083\n",
      "Epoch 18/30.... Training loss : 0.083\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.094\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.084\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.094\n",
      "Epoch 18/30.... Training loss : 0.083\n",
      "Epoch 18/30.... Training loss : 0.081\n",
      "Epoch 18/30.... Training loss : 0.084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30.... Training loss : 0.095\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.096\n",
      "Epoch 18/30.... Training loss : 0.083\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.083\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.084\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.084\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.097\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.095\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.082\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.083\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.083\n",
      "Epoch 18/30.... Training loss : 0.084\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.084\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.094\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.094\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.084\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.095\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.084\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.087\n",
      "Epoch 18/30.... Training loss : 0.091\n",
      "Epoch 18/30.... Training loss : 0.083\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.092\n",
      "Epoch 18/30.... Training loss : 0.090\n",
      "Epoch 18/30.... Training loss : 0.085\n",
      "Epoch 18/30.... Training loss : 0.096\n",
      "Epoch 18/30.... Training loss : 0.086\n",
      "Epoch 18/30.... Training loss : 0.093\n",
      "Epoch 18/30.... Training loss : 0.089\n",
      "Epoch 18/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.083\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.084\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.083\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.083\n",
      "Epoch 19/30.... Training loss : 0.083\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.093\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.083\n",
      "Epoch 19/30.... Training loss : 0.084\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.093\n",
      "Epoch 19/30.... Training loss : 0.082\n",
      "Epoch 19/30.... Training loss : 0.081\n",
      "Epoch 19/30.... Training loss : 0.084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30.... Training loss : 0.094\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.084\n",
      "Epoch 19/30.... Training loss : 0.084\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.096\n",
      "Epoch 19/30.... Training loss : 0.082\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.082\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.084\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.084\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.096\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.093\n",
      "Epoch 19/30.... Training loss : 0.094\n",
      "Epoch 19/30.... Training loss : 0.084\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.082\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.082\n",
      "Epoch 19/30.... Training loss : 0.093\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.082\n",
      "Epoch 19/30.... Training loss : 0.084\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.093\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.083\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.093\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.093\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.093\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.083\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.094\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.093\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.083\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.093\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.086\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.083\n",
      "Epoch 19/30.... Training loss : 0.087\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.089\n",
      "Epoch 19/30.... Training loss : 0.091\n",
      "Epoch 19/30.... Training loss : 0.090\n",
      "Epoch 19/30.... Training loss : 0.084\n",
      "Epoch 19/30.... Training loss : 0.095\n",
      "Epoch 19/30.... Training loss : 0.085\n",
      "Epoch 19/30.... Training loss : 0.092\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 19/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.083\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.083\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.082\n",
      "Epoch 20/30.... Training loss : 0.082\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.093\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.083\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.093\n",
      "Epoch 20/30.... Training loss : 0.082\n",
      "Epoch 20/30.... Training loss : 0.080\n",
      "Epoch 20/30.... Training loss : 0.083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30.... Training loss : 0.094\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.095\n",
      "Epoch 20/30.... Training loss : 0.082\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.082\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.083\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.096\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.093\n",
      "Epoch 20/30.... Training loss : 0.094\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.082\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.082\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.082\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.083\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.093\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.093\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.083\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.094\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.083\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.086\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.083\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.089\n",
      "Epoch 20/30.... Training loss : 0.091\n",
      "Epoch 20/30.... Training loss : 0.090\n",
      "Epoch 20/30.... Training loss : 0.084\n",
      "Epoch 20/30.... Training loss : 0.095\n",
      "Epoch 20/30.... Training loss : 0.085\n",
      "Epoch 20/30.... Training loss : 0.092\n",
      "Epoch 20/30.... Training loss : 0.088\n",
      "Epoch 20/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.082\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.083\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.082\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.082\n",
      "Epoch 21/30.... Training loss : 0.082\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.083\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.081\n",
      "Epoch 21/30.... Training loss : 0.080\n",
      "Epoch 21/30.... Training loss : 0.083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30.... Training loss : 0.093\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.083\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.095\n",
      "Epoch 21/30.... Training loss : 0.082\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.082\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.083\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.083\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.095\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.093\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.081\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.081\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.082\n",
      "Epoch 21/30.... Training loss : 0.083\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.083\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.083\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.094\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.088\n",
      "Epoch 21/30.... Training loss : 0.082\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.086\n",
      "Epoch 21/30.... Training loss : 0.090\n",
      "Epoch 21/30.... Training loss : 0.082\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.091\n",
      "Epoch 21/30.... Training loss : 0.089\n",
      "Epoch 21/30.... Training loss : 0.084\n",
      "Epoch 21/30.... Training loss : 0.094\n",
      "Epoch 21/30.... Training loss : 0.085\n",
      "Epoch 21/30.... Training loss : 0.092\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 21/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.082\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.083\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.082\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.081\n",
      "Epoch 22/30.... Training loss : 0.082\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.092\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.082\n",
      "Epoch 22/30.... Training loss : 0.083\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.092\n",
      "Epoch 22/30.... Training loss : 0.081\n",
      "Epoch 22/30.... Training loss : 0.080\n",
      "Epoch 22/30.... Training loss : 0.083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30.... Training loss : 0.093\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.083\n",
      "Epoch 22/30.... Training loss : 0.083\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.083\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.095\n",
      "Epoch 22/30.... Training loss : 0.081\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.081\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.083\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.083\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.095\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.092\n",
      "Epoch 22/30.... Training loss : 0.093\n",
      "Epoch 22/30.... Training loss : 0.083\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.081\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.081\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.081\n",
      "Epoch 22/30.... Training loss : 0.083\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.092\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.082\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.092\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.092\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.082\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.093\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.092\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.082\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.085\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.082\n",
      "Epoch 22/30.... Training loss : 0.086\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.088\n",
      "Epoch 22/30.... Training loss : 0.090\n",
      "Epoch 22/30.... Training loss : 0.089\n",
      "Epoch 22/30.... Training loss : 0.083\n",
      "Epoch 22/30.... Training loss : 0.094\n",
      "Epoch 22/30.... Training loss : 0.084\n",
      "Epoch 22/30.... Training loss : 0.091\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 22/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.082\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.082\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.081\n",
      "Epoch 23/30.... Training loss : 0.081\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.082\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.081\n",
      "Epoch 23/30.... Training loss : 0.079\n",
      "Epoch 23/30.... Training loss : 0.082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30.... Training loss : 0.093\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.094\n",
      "Epoch 23/30.... Training loss : 0.081\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.081\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.082\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.094\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.092\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.081\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.081\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.081\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.082\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.082\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.093\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.082\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.085\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.082\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.088\n",
      "Epoch 23/30.... Training loss : 0.090\n",
      "Epoch 23/30.... Training loss : 0.089\n",
      "Epoch 23/30.... Training loss : 0.083\n",
      "Epoch 23/30.... Training loss : 0.094\n",
      "Epoch 23/30.... Training loss : 0.084\n",
      "Epoch 23/30.... Training loss : 0.091\n",
      "Epoch 23/30.... Training loss : 0.087\n",
      "Epoch 23/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.082\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.082\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.081\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.081\n",
      "Epoch 24/30.... Training loss : 0.081\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.091\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.082\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.091\n",
      "Epoch 24/30.... Training loss : 0.080\n",
      "Epoch 24/30.... Training loss : 0.079\n",
      "Epoch 24/30.... Training loss : 0.082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30.... Training loss : 0.092\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.082\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.094\n",
      "Epoch 24/30.... Training loss : 0.081\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.081\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.082\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.082\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.094\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.091\n",
      "Epoch 24/30.... Training loss : 0.092\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.081\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.080\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.081\n",
      "Epoch 24/30.... Training loss : 0.082\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.091\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.082\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.091\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.091\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.091\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.082\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.092\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.091\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.087\n",
      "Epoch 24/30.... Training loss : 0.081\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.091\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.085\n",
      "Epoch 24/30.... Training loss : 0.089\n",
      "Epoch 24/30.... Training loss : 0.082\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.090\n",
      "Epoch 24/30.... Training loss : 0.088\n",
      "Epoch 24/30.... Training loss : 0.083\n",
      "Epoch 24/30.... Training loss : 0.093\n",
      "Epoch 24/30.... Training loss : 0.084\n",
      "Epoch 24/30.... Training loss : 0.091\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 24/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.081\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.082\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.081\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.080\n",
      "Epoch 25/30.... Training loss : 0.081\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.090\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.091\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.081\n",
      "Epoch 25/30.... Training loss : 0.082\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.090\n",
      "Epoch 25/30.... Training loss : 0.091\n",
      "Epoch 25/30.... Training loss : 0.080\n",
      "Epoch 25/30.... Training loss : 0.079\n",
      "Epoch 25/30.... Training loss : 0.082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30.... Training loss : 0.092\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.082\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.094\n",
      "Epoch 25/30.... Training loss : 0.080\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.080\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.082\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.082\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.094\n",
      "Epoch 25/30.... Training loss : 0.090\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.091\n",
      "Epoch 25/30.... Training loss : 0.092\n",
      "Epoch 25/30.... Training loss : 0.082\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.080\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.080\n",
      "Epoch 25/30.... Training loss : 0.090\n",
      "Epoch 25/30.... Training loss : 0.090\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.081\n",
      "Epoch 25/30.... Training loss : 0.082\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.091\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.082\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.090\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.090\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.091\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.090\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.091\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.082\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.092\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.091\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.081\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.090\n",
      "Epoch 25/30.... Training loss : 0.091\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.084\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.085\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.081\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.090\n",
      "Epoch 25/30.... Training loss : 0.087\n",
      "Epoch 25/30.... Training loss : 0.089\n",
      "Epoch 25/30.... Training loss : 0.088\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.093\n",
      "Epoch 25/30.... Training loss : 0.083\n",
      "Epoch 25/30.... Training loss : 0.090\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 25/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.081\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.081\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.080\n",
      "Epoch 26/30.... Training loss : 0.081\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.081\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.080\n",
      "Epoch 26/30.... Training loss : 0.079\n",
      "Epoch 26/30.... Training loss : 0.081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30.... Training loss : 0.092\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.093\n",
      "Epoch 26/30.... Training loss : 0.080\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.080\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.093\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.091\n",
      "Epoch 26/30.... Training loss : 0.091\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.080\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.080\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.080\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.081\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.091\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.092\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.091\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.081\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.084\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.081\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.087\n",
      "Epoch 26/30.... Training loss : 0.089\n",
      "Epoch 26/30.... Training loss : 0.088\n",
      "Epoch 26/30.... Training loss : 0.082\n",
      "Epoch 26/30.... Training loss : 0.093\n",
      "Epoch 26/30.... Training loss : 0.083\n",
      "Epoch 26/30.... Training loss : 0.090\n",
      "Epoch 26/30.... Training loss : 0.086\n",
      "Epoch 26/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.081\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.081\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.080\n",
      "Epoch 27/30.... Training loss : 0.080\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.081\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.080\n",
      "Epoch 27/30.... Training loss : 0.078\n",
      "Epoch 27/30.... Training loss : 0.081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30.... Training loss : 0.091\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.093\n",
      "Epoch 27/30.... Training loss : 0.080\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.080\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.093\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.091\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.080\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.080\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.080\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.081\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.081\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.092\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.081\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.084\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.081\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.087\n",
      "Epoch 27/30.... Training loss : 0.089\n",
      "Epoch 27/30.... Training loss : 0.088\n",
      "Epoch 27/30.... Training loss : 0.082\n",
      "Epoch 27/30.... Training loss : 0.093\n",
      "Epoch 27/30.... Training loss : 0.083\n",
      "Epoch 27/30.... Training loss : 0.090\n",
      "Epoch 27/30.... Training loss : 0.086\n",
      "Epoch 27/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.081\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.081\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.080\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.080\n",
      "Epoch 28/30.... Training loss : 0.080\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.090\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.081\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.090\n",
      "Epoch 28/30.... Training loss : 0.079\n",
      "Epoch 28/30.... Training loss : 0.078\n",
      "Epoch 28/30.... Training loss : 0.081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30.... Training loss : 0.091\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.081\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.093\n",
      "Epoch 28/30.... Training loss : 0.080\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.080\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.081\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.081\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.093\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.090\n",
      "Epoch 28/30.... Training loss : 0.091\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.080\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.079\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.080\n",
      "Epoch 28/30.... Training loss : 0.081\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.090\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.081\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.090\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.090\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.081\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.091\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.090\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.081\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.090\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.084\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.081\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.087\n",
      "Epoch 28/30.... Training loss : 0.089\n",
      "Epoch 28/30.... Training loss : 0.086\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.088\n",
      "Epoch 28/30.... Training loss : 0.082\n",
      "Epoch 28/30.... Training loss : 0.092\n",
      "Epoch 28/30.... Training loss : 0.083\n",
      "Epoch 28/30.... Training loss : 0.090\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 28/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.080\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.081\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.080\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.080\n",
      "Epoch 29/30.... Training loss : 0.080\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.089\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.090\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.081\n",
      "Epoch 29/30.... Training loss : 0.081\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.089\n",
      "Epoch 29/30.... Training loss : 0.090\n",
      "Epoch 29/30.... Training loss : 0.079\n",
      "Epoch 29/30.... Training loss : 0.078\n",
      "Epoch 29/30.... Training loss : 0.081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30.... Training loss : 0.091\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.081\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.093\n",
      "Epoch 29/30.... Training loss : 0.080\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.079\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.081\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.081\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.093\n",
      "Epoch 29/30.... Training loss : 0.089\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.090\n",
      "Epoch 29/30.... Training loss : 0.091\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.080\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.079\n",
      "Epoch 29/30.... Training loss : 0.089\n",
      "Epoch 29/30.... Training loss : 0.089\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.080\n",
      "Epoch 29/30.... Training loss : 0.081\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.090\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.081\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.089\n",
      "Epoch 29/30.... Training loss : 0.089\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.089\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.090\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.089\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.090\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.081\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.091\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.090\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.080\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.089\n",
      "Epoch 29/30.... Training loss : 0.090\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.084\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.081\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.089\n",
      "Epoch 29/30.... Training loss : 0.086\n",
      "Epoch 29/30.... Training loss : 0.088\n",
      "Epoch 29/30.... Training loss : 0.087\n",
      "Epoch 29/30.... Training loss : 0.082\n",
      "Epoch 29/30.... Training loss : 0.092\n",
      "Epoch 29/30.... Training loss : 0.083\n",
      "Epoch 29/30.... Training loss : 0.090\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 29/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.080\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.081\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.080\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.079\n",
      "Epoch 30/30.... Training loss : 0.080\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.080\n",
      "Epoch 30/30.... Training loss : 0.081\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.079\n",
      "Epoch 30/30.... Training loss : 0.078\n",
      "Epoch 30/30.... Training loss : 0.080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30.... Training loss : 0.091\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.081\n",
      "Epoch 30/30.... Training loss : 0.081\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.092\n",
      "Epoch 30/30.... Training loss : 0.079\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.079\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.081\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.081\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.092\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.090\n",
      "Epoch 30/30.... Training loss : 0.090\n",
      "Epoch 30/30.... Training loss : 0.081\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.079\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.079\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.080\n",
      "Epoch 30/30.... Training loss : 0.081\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.090\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.081\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.090\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.081\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.091\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.090\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.080\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.090\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.083\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.081\n",
      "Epoch 30/30.... Training loss : 0.084\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.086\n",
      "Epoch 30/30.... Training loss : 0.088\n",
      "Epoch 30/30.... Training loss : 0.087\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.092\n",
      "Epoch 30/30.... Training loss : 0.082\n",
      "Epoch 30/30.... Training loss : 0.089\n",
      "Epoch 30/30.... Training loss : 0.085\n",
      "Epoch 30/30.... Training loss : 0.085\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        for ii in range(0, len(train_images), batch_size):\n",
    "            batch_images = train_images[ii : ii + batch_size]\n",
    "            feed = {\n",
    "                inputs : batch_images,\n",
    "                targets : batch_images\n",
    "            }\n",
    "            _cost, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            print(\"Epoch {:}/{:}....\".format(epoch+1, epochs), \n",
    "                  \"Training loss : {:0.3f}\".format(_cost))\n",
    "    saver.save(sess, \"./autoencoder.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./autoencoder.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABawAAAEsCAYAAAAvofT2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3XncndO5MP47QoREYggxRsxDap4jxkPLQU5RQ42lhlKcarU1T6U1D1U1lRpKtbSGqumUxDHTmKeSICLGiBCJISK/P963n997r2vF3tnD89zJ8/3+d12ufe+VZ699D8v+rKvbtGnTCgAAAAAA6GyzdfYAAAAAAACgKCxYAwAAAABQERasAQAAAACoBAvWAAAAAABUggVrAAAAAAAqwYI1AAAAAACVYMEaAAAAAIBKsGANAAAAAEAlWLAGAAAAAKASZp+R4n79+k0bOHBgm4bCzG7EiBHjpk2btuD0/rv5w/SYOzTD/KEZ5g/NMH9ohvlDM8wfmmH+0Azzh2bUmj//NkML1gMHDiz++c9/Nj4qZmndunUb/XX/3fxheswdmmH+0Azzh2aYPzTD/KEZ5g/NMH9ohvlDM2rNn3+zJQgAAAAAAJUwQ7+w/n9169atleNgJjVt2rSGXmf+UBTmD80xf2hGI/PH3KEonHtojvlDM8wfmmH+0Azzh2Y0Mn/8whoAAAAAgEqwYA0AAAAAQCVYsAYAAAAAoBIsWAMAAAAAUAkWrAEAAAAAqAQL1gAAAAAAVIIFawAAAAAAKsGCNQAAAAAAlWDBGgAAAACASrBgDQAAAABAJViwBgAAAACgEixYAwAAAABQCRasAQAAAACohNk7ewAwMzn99NNDbu655w65tddeuxSvv/76dR3/1ltvLcXDhg0LNeedd15dxwIAAACAmY1fWAMAAAAAUAkWrAEAAAAAqAQL1gAAAAAAVIIFawAAAAAAKkHTRfgaDz74YCneYIMNGjrOtGnT6qrbbrvtSvGGG24YatLGjEVRFK+++mpD42LWtsoqq4Tc008/HXK/+MUvSvEJJ5zQtjHRfr179y7F1157bahJzzVFURRvvPFGKf6P//iPUDNq1KgmRwcAAF3DAgssEHIrrLDCDB/npZdeCrlTTz015NJnvWeeeSbUPPTQQzP8/tAZ/MIaAAAAAIBKsGANAAAAAEAlWLAGAAAAAKAS7GEN/1e6X3VRNL5n9XvvvVeKhw0bFmqWXXbZkFtrrbVK8fzzzx9qDj300JA7/PDDZ3SIdAEbbbRRyOX2Ux8zZkxHDIcOMnDgwFK87bbbhprcPBgwYEAp3mOPPULNSSed1Nzg6BQbb7xxyOX6Icw777wdMZzp2nXXXUPu0UcfLcWvvfZaRw2HTrL33nuH3JVXXhlyJ554Yik+5ZRTQs3UqVNbNSzqtMgii5Ti4cOHh5oHHngg5E477bRS/Morr7R0XK0w33zzhdzQoUND7rrrrivFU6ZMaduYgM6z5557luLcfcy6664bcrl9rWsZN25cyOXu22afvfYS32yz+d0qMwczFQAAAACASrBgDQAAAABAJViwBgAAAACgEixYAwAAAABQCZou0iVtttlmIbfeeuvVfN0777wTcptssknNuokTJ4aaHj16hNyoUaNK8WKLLRZqFlpooZrjhKIoinXWWSfkco1/fve733XEcGiDhRdeOORuueWWThgJVfbtb3875Lp3794JI/l6u+yyS8gdcsghpXjIkCEdNRw6SHpfc8EFF9T1urTp4plnnhlqJk+e3PC4qC3XOGzkyJGleM455ww1ueZhM0OTxfTfVhRF0atXr5AbMWJEKX7uuedaO7AuLtdoLm3MuvLKK4eaQYMGhZyGmBRFUay00kql+Pjjjw81O+ywQ8ilDQ67devW2oH9P/r169e2Y0NV+YU1AAAAAACVYMEaAAAAAIBKsGANAAAAAEAlzDR7WB9wwAGl+NBDDw017777bsile9ddeumloebVV18NuRdeeGFGh8hMZMCAASGX23Mq3Ys6t8/1mDFjGhrD6aefHnK5/WhTf/3rXxt6P2Z96fzcbbfdQs2dd97ZUcOhxU4++eSQ22mnnUJu4MCBLXm/b37zmyE322zx/3M/8cQTpdge2p0v3VNxu+2266SRzJgHHngg5H7yk5+U4t69e4eaTz75pG1jov3S+TnPPPPU9br777+/FH/66actGxNR//79Q2748OEhN9dcc5Xim266KdTsuOOOLRtXO6X7qad7WhdFURx11FEhZ8/q1jnssMNCLnc/1KdPn5rHyn1+7733XmMDY5aywgorlOJcT42Ols7N3JoV1ZTbQ3+JJZYIufRZPdcb7auvvgq53/zmN6X47rvvDjWzynXIL6wBAAAAAKgEC9YAAAAAAFSCBWsAAAAAACrBgjUAAAAAAJUw0zRdTBvU9e3bN9QMGjSo5nG23XbbkPviiy9CbuzYsTMwuo6RNpU85phjQs2wYcM6ajgztauuuirkcs2ePvroo1I8bty4lo1h5513Drnu3bu37Ph0PauttlopnmOOOULN73//+44aDi127LHHhty0adPa9n7rr79+XbkJEyaU4lwzrVxjLton/QyWXnrpUHPllVd20Gjq169fv5BLG71pujhz69mzZ8idcMIJDR3rkksuKcXtPB9SFJtttlnIpY3Kcn74wx+2Yzgtt/baa4dc2hDrscceCzUXX3xx28bUFaWNo3/1q1+FmrSxZ71uvPHGkNthhx1KcSuf9WivXCPYU045pRTn1kauu+66kPvss89K8eeffx5qcmtGPXr0KMUjRowINWlz8qIoigcffLAU5+6TJ02aVIrd61TDeuutF3LpM9rmm28eaho9b+WcddZZpTjXmPH9998vxY8//nio+c53vhNyuXnemfzCGgAAAACASrBgDQAAAABAJViwBgAAAACgEixYAwAAAABQCTNN08UDDjigFK+55pqh5tlnnw25VVZZpRRvsMEGoWaNNdYIuaWWWqoUf/zxx6GmT58++cHWkNsUffLkyaU411QoHdN+++0XajRdbNyoUaPaduwzzjgj5BZaaKGar3vttddC7s4772zJmJj1HH300aU4bRpaFEXxj3/8o6OGQ5OeeuqpUtytW7e2vt+nn35ainNNN3INj+ebb75SfO+994aa2Wbz/8fbJdf8JW2uOn78+FDzox/9qG1jalTa/IpZz+DBg0NuiSWWqPm63L3ztdde25IxkbfIIouU4j333LOu1/30pz8txe+8807LxtRKaZPFep6h/vjHP4Zc7l6LxqXPTK1sVDZkyJCQGzNmTCk+//zzQ83xxx8fclVrTDary62N/POf/wy5xRZbrBSnzQ2nJ32+XnXVVUPNK6+8EnJpU+vXX3891OSuX1RT2lz+uOOOCzW5hopzzjlnzWNPnDgx5J5++ulS/PLLL4eaffbZJ+TeeOONUrzkkkuGml69epXijTfeONT87Gc/C7m0cWln8wQJAAAAAEAlWLAGAAAAAKASLFgDAAAAAFAJM80e1jfccMPXxs1YYIEFQm6zzTYrxbl9X7fccsuG3i/dr7ooimLEiBGl+NVXXw01PXv2LMX/+te/Gnp/2m+vvfYqxYcffnio6d69e8hNmjSpFP/kJz+pWUPXtNxyy4XcgAEDSvG4ceNCzSeffNK2MdG4b3/72yGXfp7Tpk0LNblcPW6++eaQu/XWW0vxhAkTQs23vvWtkDvwwANrvl+6B9wvfvGLmq+hPmeffXbIzTHHHKV4l112CTW5vfQ6Wr9+/Urx8ssvH2oaneNUU737IKeeeeaZFo+EWtL9mjfZZJNQk+7/WxRFcckll7RtTK201VZbleJ0v8+iKIp77rmnFOf2N6ZxyyyzTMgNHTq05uvefvvtkEt7NQwaNKiuMaR7z/7whz8MNRdccEHIjR07tq7j05gePXqU4uHDh4eadL/qoiiKyy+/vBQ3umaU2686J7dmw8zh73//e8htuummpbjePfRffPHFUpy7Z9l3331DLu0flJPbe3/XXXctxX/5y19CTdofJLeGdPLJJ4fc7373u1Lc2X0o/MIaAAAAAIBKsGANAAAAAEAlWLAGAAAAAKASLFgDAAAAAFAJM03TxXb64IMPQu7GG2+s+bpWNn7cf//9S3HaYLEoYoOJ3/72ty17f1pr/fXXL8W5Bos5d9xxRynONUaDoiiK7bbbrmbNRx991AEjYUblGmZec801ITf33HM3dPy0WeJtt90Wag4++OCQq6eh63PPPRdyaRO13LiPPfbYUpxrYnLCCSeE3JQpU2qOqSs54IADQm7ttdcOubTh6r333tu2MTXj17/+dSnONVhMG0zn7tmYeWy88cY1a6ZOnRpyhxxySDuGw9dIv4+57+f7778fcp9//nnbxlSP3DXovPPOC7k99tij5rG23HLLloyJvNz5IG22N3LkyFCTa9Cb3lfkzhlHHnlkyM0333yluHfv3qHmwQcfDLn02ptrdE595plnnpA799xzS/Gaa64ZaiZPnhxyP/vZz0pxPfe2zHrS88GZZ54Zarbeeuuax8nNsauvvjrk0nn3ySef1Dx2vfr06RNys89eXsY95phjQs11111Xivv27duyMXUkv7AGAAAAAKASLFgDAAAAAFAJFqwBAAAAAKgEC9YAAAAAAFSCpoudYJFFFgm5tLFAt27dQs2JJ55YijV3qIbHH3885FZbbbWar8s1wfr+97/fkjEx61trrbVq1pxyyikdMBJm1JxzzhlyjTZYTBvSFUVRbLbZZqX43XffbejYOaNGjQq5c845pxSnDRaLoijmmGOOUvzzn/881OQaT7744oszOsRZ2t577x1y6d+2KIrioosu6ojhzJBcs9GhQ4eW4q+++irUHHfccaVYI86ZR66h0dJLL13zdbnPONf0jM63xhprhNyzzz5bij/++ONQk143mrHFFluU4vQaWBRFsdRSS9U8zsMPP9yyMVGfnj171qw57bTT6jrWp59+WopzTdZ23333kEubLuaai3722Wch19nNRWcl++67b81crpF87vzz4Ycftm5gzLS23377Urz//vvX9bq0WeIOO+wQav7xj380PrBE9+7dS3HuHin3fJSOoZ5zaW59cfjw4SFXtebmfmENAAAAAEAlWLAGAAAAAKASLFgDAAAAAFAJ9rDuBMcff3zIpfuX5vbKevrpp9s2JuqzxBJLhNzKK68ccrPPXv5qTZ48OdQceuihITdx4sQmRsesaquttgq5dG+uoiiKN998sxT/+c9/btuY6HhvvPFGyG277bYh18o9q+tx9dVXl+K99tor1Cy55JIdNZxZSrq35qBBg+p63cknn9yO4TTlyCOPDLm55pqrFL/33nuh5sYbb2zbmGivwYMHN/S6a6+9tsUjoREnnXRSKb711ltDTe/evUNu+eWXr3ns6667rvGBtUi61+1+++3XSSPpuvbZZ5+aNTvttFPIXXHFFQ29X66XQj1y+5t7ZmudzTffvGbNyy+/HHKvv/56G0bDrCDdGzrXIyVn6tSppXijjTYKNbnnnHruz3Pre2l/hf79+4ea3DpSr169ar5fatKkSSF32GGHhVzVesX4hTUAAAAAAJVgwRoAAAAAgEqwYA0AAAAAQCVYsAYAAAAAoBI0XWyzbbbZJuT233//mq/bddddQ+6xxx5ryZho3PDhw0MubRqVk2tU8+KLL7ZiSHQB//mf/xlyuXn32muvleJPP/20bWOitbp161azZuDAge0fSANmm638/75z/5Z6/n0XX3xxyG2yySaND2wW0LNnz1I8zzzzhJoHHnigo4bTlBVXXLFmzciRIztgJHSUjTfeuK66tBHRKaec0o7hMIPSe960OVRRFMWmm24ackOHDi3Fe+65Z6jJNZH6y1/+MmMD/L8uvPDCUvzII4/U9bq0mb378o73+9//PuTWXnvtUrzqqquGmtVXXz3k1l9//VK82267hZr0mloU8fyTq9lll11C7je/+U0pHjFiRKihPltssUXNmjXWWCPk0u9+URTF9ddfX4rvv//+xgfGTCu9nhx66KGhZrXVVgu5vn37luLjjz8+1EybNq3m++dq6nkWyqmnwWLu/dK1w5133jnUjBkzpqExdSS/sAYAAAAAoBIsWAMAAAAAUAkWrAEAAAAAqAQL1gAAAAAAVIKmi222/fbbh1zaoKooYqOP22+/vW1jon7f+973SvGAAQPqet2//vWvUnzggQe2akh0Qeuss07I5ZorXH311R0xHJp01FFHhVw9DTyqao899ijFSyyxRKhJ/325f+8PfvCD1g5sFvDRRx+V4rFjx4aaZZddNuT69etXiseNG9fagdWwyCKLhNwGG2xQ83X/+Mc/2jEcOsi2225bijfaaKO6Xvf555+X4tdff71VQ6KFPvjgg5DLNUpMc3vvvXfbxlQU9TV0zZ07c0356Fg33HBDyJ1zzjmlOHc9eeKJJxp6v+effz7k0oaKabPRoojX1KIoihNPPLEUb7fddg2NiaKYe+65Qy69T5x99rhsddBBB4Vcei958803h5r77rsv5NLG5i+//HKoefzxx0MulXtmu/POO0POda690sa+6667bqiZf/75Qy49/2y44YahZsKECSE3evToUjzXXHOFmpVXXjnkllxyyZBrxG233RZy++yzTykeP358S96ro/mFNQAAAAAAlWDBGgAAAACASrBgDQAAAABAJdjDusXSPZi++c1vhpqpU6eG3BFHHFGKp0yZ0tqBUdNCCy0UcieccEIp7t69e13HevLJJ0vxxIkTGx8YXc5iiy1WildZZZVQk9uT9vLLL2/bmGid3HWhihZeeOGQW3/99UPuxz/+8QwfO91brijiPrbEv9OYMWNCTe4zeeyxx0rxGWec0bIxrbbaaiGX7su36KKLhpp69mmfmfdypygWXHDBUtytW7e6Xvfwww+3Yzh0ERdeeGHNmvQ5qyiK4p133mnHcJgBuXvZdM/zq666KtT07Nkz5NLrR25/9b322ivkPv3001L8t7/9LdSke8EWRVEMGTKkFK+00kqhJu1RRd61114bco3uMZ9ed3L9xHK5dsrd8z711FOlOJ1PtF9uT+e0f1krDRs2LOTq2cP6iy++CLnjjz++FJ999tmhJrfmODPyC2sAAAAAACrBgjUAAAAAAJVgwRoAAAAAgEqwYA0AAAAAQCVouthiaWOjxRdfPNQ888wzIXfHHXe0bUzU51e/+lXI1bMRftrcqiiK4sADD2zJmOia0iZ2aTPXoiiKRx55pKOGQxf161//OuR23HHHho41YcKEUpxravLqq682dOyu5JBDDgm5XMOxtddeu2ZNo9IGVUURm13lzln1OOussxp6HdVQT7Oizz77LOTOPPPMNoyGWdEPfvCDkNtss81Kca5B1dtvv922MdFaf/7zn2vW7L///iGXNnA84IADQk3u+pU69NBDQy7X/Lye6+zmm29e8/2IjTaLoiiuuOKKUpybF927dw+5Pn36lOJ6m/+2U+6eaIMNNijFuXvuww47rG1jor1y9zUbbbRRQ8f66U9/GnIXXHBBQ8eaGfmFNQAAAAAAlWDBGgAAAACASrBgDQAAAABAJViwBgAAAACgEjRdbMKee+4ZcgcddFAp/vzzz0PNkUce2bYx0bi99tqrodfttNNOITdx4sRmh0MXttxyy9Wsef/99ztgJHQlTz31VCkeMGBAy449evToUnzrrbe27NhdyZNPPhlygwcPDrm0sctKK63UsjFceumlNWvuvffekNtkk01qvm7y5MkNjYmON3DgwJCrp6FQ2oC1KPLzBXLqafz76KOPhtz//u//tmM4dIBcs716GjM2Kncduuqqq0Iubbq41lprhZp+/fqV4rQxJP/H1KlTQy69LqR/y+lJn8vnmGOOUHPqqaeG3JJLLlnX8VslbQa5/vrrd+j701o///nPS3Gueetss9X+rfC7774bcpdddlnjA5sF+IU1AAAAAACVYMEaAAAAAIBKsGANAAAAAEAl2MO6TgsttFDInX/++SGX7kf0+OOPh5o777yzdQOj0/Xv3z/kvvjii5Yce/z48SE3ZcqUkEv355p//vlrHnvBBRcMudyeXvX48ssvQy7dE3zSpEkNHbsr2nTTTWvW/OUvf2n/QGiL9DoxvVxq9913r+v4F110USnu3bt3Q+OaNm1aXa+rxxprrNGyY1Hb/fff/7Vxu7344oshV88e1uutt17I5fajpfNtvfXWIVfPeey2225rx3DoInL7vKb3xccdd1xHDYcuIr2vKoqi2GWXXUrxkCFDQs2JJ55Yig855JCWjovohhtuqFmT22/88MMPL8VfffVVqLnjjjtC7uyzzy7FJ510Uqipp78DM48tttgi5NLPvUePHnUdK10z2m+//ULNZ599NgOjm/X4hTUAAAAAAJVgwRoAAAAAgEqwYA0AAAAAQCVYsAYAAAAAoBI0XZyO7t27l+Jc88R555035D788MNSfOCBB7Z2YFTOY4891rZjP/TQQyH35ptvhtyiiy5ainONPzraL3/5y1L83//93500kmobOnRoyPXq1asTRkJHufTSS0Pu5z//ec3XXXPNNSFXT2PERpsnNvq6m2++uaHXMetotLGoBoszj379+tWsmTx5csgde+yx7RgOs6DcXMndH6Xz7H//93/bNia6plwDvqOOOqoUDxs2LNQcfPDBpfiSSy4JNc8++2yTo2NG3XLLLSGXNl2cbbb4u85tttkm5JZZZplSvMIKKzQ0prFjxzb0OjrezjvvHHL1NFlMGwQXRVHstttupfjvf/974wObRfmFNQAAAAAAlWDBGgAAAACASrBgDQAAAABAJdjDejpWXnnlUrzEEkvU9bof//jHpfjFF19s2ZhoryeeeCLk1llnnU4Yyf9v8ODBLTtWuv9avfvTpnt0P/jgg3W97t57761vYF3crrvuGnLpXq+5fctvuummto2J9rr88stD7tBDDw25ueeeuyOGM125/Wdzc3GHHXYoxW+88UbbxsTMIXd9aXRPdKop138h9cEHH4Tc+PHj2zEcZkEHHXRQXXW5fi+pvn37htwCCyxQil999dX6BgZFfB4655xzQs3PfvazUnzZZZeFms033zzkcvdftM4///nPkEs/zw033LCuY6244oo1a3J7oKfrDnvuuWdd70fHyl079t1334aOdffdd4fcX//614aO1ZX4hTUAAAAAAJVgwRoAAAAAgEqwYA0AAAAAQCVYsAYAAAAAoBI0XSyKYplllgm5+++/v+brzjjjjJC7+uqrWzImOt56660XcmeeeWYp7tGjR0PHXmONNUJuyJAhDR3rrrvuCrmXX3655uuuvPLKUvzkk0829P40rlevXiG3xRZb1HzdjTfeGHJTp05tyZjoeKNGjQq5PfbYI+TShpy77LJL28aUc9ZZZ4XcSSed1KFjYOZUb8PQL7/8ss0joRXmmGOOkFt88cVrvm7KlCl15aAZ6XnksMMOCzVHHHFEyI0cObIU55rfQb3OO++8kNtvv/1K8brrrhtqVl111ZB75JFHWjcwglxTy/Qe++9//3uoWXbZZUMufbabMGFCqLn++utD7uCDD645TjrePPPMU4rHjBkTamabrfZvft9+++2Q23nnnRsfWBfmF9YAAAAAAFSCBWsAAAAAACrBgjUAAAAAAJVgwRoAAAAAgErQdLEoiqOOOirk+vTpU/N1ueZ306ZNa8mYqIaf/vSnnT0EZiFffPFFyE2cODHkRo8eXYqPO+64to2Jarjllltq5v72t7+Fmv/+7/8OubXXXrsUP/7446Hm/PPPD7lu3bqVYk1/aNROO+0Ucp9//nnInX322R0xHJr01Vdfhdzzzz8fcgsvvHApTq9l0A5bbbXV18ZFURR33nlnyP3whz9s25joet55552QS5sspo0+i6IoTj/99JDbZJNNWjcw6vLWW2+V4jXWWCPU/OhHPwq5TTfdtBQfdNBBoSbXgI9q2nHHHUtx2oSxKOpb78s9n3366aeND6wL8wtrAAAAAAAqwYI1AAAAAACVYMEaAAAAAIBK6HJ7WA8dOjTk9thjj04YCdDVTJkyJeSWWWaZThgJM6Prrruurhx0tpdffjnkfvnLX4bcjTfe2BHDoUlTp04NuX333TfkLr/88lL8wAMPtG1MzPpye8Hm9vsdNmxYKT7llFNCzbhx40Iu11cEWunVV18txS+88EKoWX/99UNurbXWKsUjRoxo7cBoyHnnnVdXjpnXqaeeWorr7U93zTXXlGL3t63jF9YAAAAAAFSCBWsAAAAAACrBgjUAAAAAAJVgwRoAAAAAgErock0XN91005Dr0aNHzdd9+OGHdeUAALqyNddcs7OHQJu98cYbIbflllt2wkiYVd1666115WBmMWTIkJB77bXXQm6VVVYpxZouQsfo3bt3Ke7WrVuomTRpUsgde+yxbRtTV+cX1gAAAAAAVIIFawAAAAAAKsGCNQAAAAAAlWDBGgAAAACASuhyTRfr9dZbb5Xi1VdfPdSMGzeuo4YDAAAAzIQmTJgQcvPNN18njATIufDCC0vxUUcdFWrOOuuskBszZkzbxtTV+YU1AAAAAACVYMEaAAAAAIBKsGANAAAAAEAldLk9rH/84x/XlQMAAAAAZm1HH33018Z0PL+wBgAAAACgEixYAwAAAABQCRasAQAAAACoBAvWAAAAAABUQsNNF6dNm9bKcdDFmD80w/yhGeYPjTJ3aIb5QzPMH5ph/tAM84dmmD80yi+sAQAAAACoBAvWAAAAAABUQrcZ+Xl+t27d3i+KYnT7hsNMbslp06YtOL3/aP7wNcwdmmH+0Azzh2aYPzTD/KEZ5g/NMH9ohvlDM752/vzbDC1YAwAAAABAu9gSBAAAAACASrBgDQAAAABAJViwBgAAAACgEixYAwAAAABQCRasAQAAAACoBAvWAAAAAABUwuwzUtyvX79pAwcObNNQmNmNGDFi3LRp0xac3n83f5gec4dmmD80w/yhGeYPzTB/aIb5QzPMH5ph/tCMWvPn32ZowXrgwIHFP//5z8ZHxSytW7duo7/uv5s/TI+5QzPMH5ph/tAM84dmmD80w/yhGeYPzTAOCQhjAAAgAElEQVR/aEat+fNvM7RgnbxBoy9lFjJt2rSGXmf+UBTmD80xf2hGI/PH3KEonHtojvlDM8wfmmH+0Azzh2Y0Mn/sYQ0AAAAAQCVYsAYAAAAAoBIsWAMAAAAAUAkWrAEAAAAAqAQL1gAAAAAAVIIFawAAAAAAKsGCNQAAAAAAlWDBGgAAAACASrBgDQAAAABAJViwBgAAAACgEixYAwAAAABQCRasAQAAAACoBAvWAAAAAABUwuydPQDoDN26daurbtFFFy3FK6+8cqg57bTTQm755ZcvxZMmTQo1zzzzTMgdf/zxpfipp54KNZ999ll+sFCHeub+tGnTOmAktMscc8xRipdaaqlQs8wyy4TcI488UoonTJgQaswNAABor/SZLfcM99VXX3XUcKBT+IU1AAAAAACVYMEaAAAAAIBKsGANAAAAAEAlWLAGAAAAAKASNF2kS5httvL/m+nfv3+o2WSTTULu5JNPLsUDBgwINT169Ai5tClC7969Q80WW2wRcptuumkpfv7550PN5ptvHnIffvhhyDFrS+d0URTFvPPOW4p33nnnUPNf//VfIXfLLbeU4quvvjrUTJ48eUaHSAeYffZ4GU8/44suuijUzDnnnCH34osvluLvfOc7oWbMmDEzOkQ6QXoN6t69e6hJm3PmXpdr8JM7F3RkM87cmHK5esakiWg1pJ9fbm726dMn5NKG1rmm1D7j9sp999LrS64p2JdffhlyM0PzsNy/Nzdf03/fzPBvg1lBFZvL13PeWGWVVULNmmuuWYrT57yiKIpFFlkk5B588MFSfMcdd4Qaz3XMLPzCGgAAAACASrBgDQAAAABAJViwBgAAAACgEuxhTZeQ7lX1jW98I9QcffTRIbfEEkuU4tw+dbl96aZMmVKzJrefVbovcW7P7Nxe2zfffHPIdaTcfsrp39w+kq2V+3um+xmvv/76oWbVVVcNubfeeqsU5/awpppye+jvvvvupXieeeYJNbnv7HLLLVeKN9hgg1CTzpWiKIqpU6fWHCftk7uWpPsc5q4lAwcODLn333+/FI8cOTLUfP755yGXzoF6z/fpNbVfv36hJs3l5mC6l3FRFMUXX3zR0JjoeOk8SM9hRVEUxx57bMi98sorpfi73/1uqNHjo3HpuWXhhRcONQceeGDIbbPNNqV4woQJoWb48OEhd9lll5Xi9HxUFB3/PU7vq3LXxeWXXz7k7rnnnlI8evToUOOc1LhcX4ZU7tmrin/zevoy2AO9/v4VqdxnXs9za87cc88dcun9Vu7Za9tttw259DzZt2/fmuOsd05vt912pfjll18ONc8++2zIQRX5hTUAAAAAAJVgwRoAAAAAgEqwYA0AAAAAQCVYsAYAAAAAoBJmmqaLaXOFXPO7+eabL+QmT55cij/99NNQkzbIK4pqNmWgcXPOOWcp3nzzzUPN4osvHnLpPHj99ddDzZVXXhlyf/rTn0rxxIkTQ832228fcsccc0wp7tOnT6hJGykURVHceuutpbidzTnqbXqRNoP77LPP2jamrih3jkrPk7kGSbnmZOn8yZ0n6Xy579lPf/rTkPvWt75VinPXy9z8mWuuuUrxz3/+81Dz3nvvhdwDDzxQir/88stQQ/vkmgf179+/FG+11VahplevXiGXngty1656ri/1NBUuinjd3WeffULNoEGDSvENN9wQam677baQS5tDuq+rrvRa9atf/SrU5Bpyps2uaK30HnT//fcPNbnrRHovkjZAnZ6//vWvpXj8+PGhppXXl/Q8tdBCC4Wa8847rxTnmrZfc801IffRRx997XsVhXPS9KR/q6WXXjrUHHnkkSGXnkdyn8sdd9wRcp988kkp7ujPJX1GzeVy36FcA+RZueF9vc0TU7kGnblc+jfPNUr89a9/HXLpebKeMdUr/S7kxp37uyyyyCKl+De/+U2o2XLLLUOu3nM1dCS/sAYAAAAAoBIsWAMAAAAAUAkWrAEAAAAAqIRK7GGd7vWz4IILhpqtt966FK+22mqhZq211gq5dB/He++9N9Q8+OCDIffUU0+V4jfffDPU5PZR6927dynO7WM0derUkEv3oUr3/829X27vqtzes+3cz3hmke5LN/vsceqPGTMm5O66665SfNppp4WaDz/8MOTq2TMs3ee6KIriF7/4RSnu2bNnqNlwww1DLv33tHMPqty/LTencznaa9llly3FuX3ZR48eHXLpeXFW2vNuVrLOOuuEXG7/0Nx+iPVI97peZZVVQs3tt98ecuk1dOjQoaHGvujtk/u8d9hhh1K87rrrhpqHH3445F577bVSnOs90Oj5Ife6dH/G3JwbMGBAKc7tzV/v+9H5cnv57rfffqU4t1917n463WP9448/bnJ0XVfuc0n3Qd19991DTe5+Oj1W7jyS7lddFLFPTLvvI9Oxf/e73w016f7/uX21H3vssZBL56Jnsfql5/x77rkn1Cy22GI1jzN48OCQy/U7OPfcc0vx888/H2py/a5aJXfs9H4sd53Pva6r9RDJnbfSv1WuX0fas6Uo4nw58cQTQ02ub0J6bar3OTm9L86t66R9h3Ln29yxx40bV4qPPvroUGO/6vrU27srnXe59cxcn5bddtutFM8///w1j10Uce/9hx56KNQce+yxpXjkyJGhZmY4Z/iFNQAAAAAAlWDBGgAAAACASrBgDQAAAABAJViwBgAAAACgEirRdDGV28i8e/fupTjXSCFt0lAUcaP9HXfcMdRss802IZc2O8g1y5hvvvlqvi638X6u+ch7771XiueZZ55Q8/7775fiiy66KNRcfvnldb3frKyexjx33313qLniiitCrp4GVPXIzemLL7445Pr06VOKc/MnbQRZFO1tBkI1pefEoiiK7bbbrhQvuuiioeayyy4LufT7QTWkTWFyjYdyjVnT8029zefS61zuvJVe44qiKIYMGVKKr7766lCz5557luKudl1qp7SpcFEUxbe+9a1SnGvicv3114dcei5oZePC3H3Uu+++W4onT54catL7ody8NJ9mHrlzyPe+971SnLu+5ebPUUcdVYo1e25c7rueNuXKNYTPfS5pM7Hc/W7uOpF+j+tp1Dq9unqk82XppZcONWmTsw8++CDUPProozWPTV7uu57epy6xxBKhJvesl86D9JmqKPLrAJtsskkpzjWou+GGG0KuVc9eubnivjzKfea560l6z5CryTU4fOmll0rxK6+8EmpyzezTY+XWE3LnwLfeeqsU55qTp+fX3L8ld15Ox2SdoH7pPMutL2688cYhl953b7311qEmd07KXdPqkT4j5tYzV1xxxVJ8+umnh5rcua1q99R+YQ0AAAAAQCVYsAYAAAAAoBIsWAMAAAAAUAkWrAEAAAAAqIRKNF1MmySMHz8+1AwbNqwUL7DAAqFm9dVXD7mFF164FKcblBdFvmHRnHPOWYpzja3SRhxFETdqz21yP27cuJDr3bt3Kc41dEybCKy00kqh5ssvvwy5ribXAOajjz4qxffdd1+oyTW9aFXTlI022ijkhg4dGnLp/BkxYkSoSZsMFUVrG2PROvU0Umj0s8s1Zv3ud79binPnrSeeeKKh96Pj3X///aU4vU40I3duS69XublZz3Vvq622CjXHHHNMKT711FNDTdWafFRR7u+/6667htxSSy1Vij/++ONQ8/zzz4dcPY03cxo9j6WNsVdYYYVQk57HRo0aFWpy130a12jj1noMHDgw5Pr371/zdbnzw6233tqKITEdaROw3GeQO7c8+eSTpfiSSy4JNbkGq+k8y51/Wtl0MT3/pA2EiyI2nrzqqqtCjWtX43LPu+uss04pzjXbq0duXuSaPKZrBbn7k9x157HHHqv5frROv379Qi5tKlcU8Zn/jTfeCDWffPJJyKXrMzvvvHOoyTU4rOc82aq5kZ6PppejPrl76r322qsUH3nkkaFm3nnnDbl07TCNiyJ/r5p+frnmm7lntvT4uWOnDSNPO+20UDN27NiQGz58eM1jdyS/sAYAAAAAoBIsWAMAAAAAUAkWrAEAAAAAqIRK7mGd24vntddeK8XnnXdeqMntS5XuWZ3bwzq3H9Ecc8xRihdccMFQk+4LUxRF8fbbb5fil156qa73u/zyy0vxZpttFmrSPbzS9yqKzt9jpqrSv0sr93vK7aeXzo0bbrgh1OTma7oXYLqPUlHk9/1j5tDK/e3WWGONkEv348/Nldy+tXS+3Dk/9xnXI51nub36ctemV199tRT37ds31KR7PRZFUSy55JKlOLdv22GHHVaKc3v85fZWa1UfgVlF+rcuiqLYfffdQy7d7/yBBx4INbl9Hdv5987tQ5qOPdebI50rr7zySqhx71NNufujfffdN+TSe+7ctTLd078oimLSpElNjI5acn0wUrneOc8880wpzvUmavR+KHceqef7n3v2Su/Nc/vhvvnmm6X4pptuCjX2Lq5P7nyw8sorh1z6fJT7fHO59Blq9OjRoWaxxRYLuT59+pTi3DN/rn9Q2jcmt/csjUv3Nz/llFNCTe57fdlll5Xi3D1wrsdYKvc6Zl65fadvvvnmkFtvvfVKce68lbv3ePjhh0vx008/HWrSa2NRFMW9995binPnkVzvvm233bYUH3LIIaEmXY+ae+65Q80ZZ5wRcv/xH/9RitN94TuaX1gDAAAAAFAJFqwBAAAAAKgEC9YAAAAAAFSCBWsAAAAAACqhEk0X61FPY8acVjVAePnll0PuwQcfbOhYuQYB88wzT83XpY1N/vCHP4QajYfaK7fxftqsoyhiw4dcTW4O33LLLaU41xSLmUerGvHkmgwNHjw45NJGNWmzoKLo/MYJ5BuuXnXVVSGX+9xTuTk2duzYUrzffvuFmhdffDHk0vNbrnniKqusEnJHHnlkKV5++eVDTdro44gjjgg11113XcilDZe7mnQO7LbbbqFm8cUXD7m04eqFF14YatIGVUXR3uZhuabXxx57bCnONXn74IMPSrHrYvu1ah6kzRSLoih22GGHkEvPPblGfrlzBq2Tu96k55ZevXqFmlxu0KBBpTj3vc49n6XzIHetzD1Dpc8+uea0f/zjH0PuG9/4Rs0xpc32cg3vqU/u80w/g6KI55/c53LnnXeG3NVXX12Kc9eKgw8+OOTS62pujm244YYhlzYF1HSxcbPPHpekzjnnnFK80047hZrc9/Hss88uxbnrSTvl1go0Zu146f3HX/7yl1AzZMiQkEuvhZ9//nmoyTWJv+KKK0rxhAkTQk0963S5+ZOz6KKLluL+/fuHmtw9WCr3zJYeu7PXDvzCGgAAAACASrBgDQAAAABAJViwBgAAAACgEixYAwAAAABQCTNN08VZSb9+/UIubWQ1derUUHPJJZeUYo2HOl6uKcRGG20UciuuuGIpzm2y/95774Xcvffe28TomFXl5t13vvOdkEsbi6QNIIpCY9YqWHrppUNu4YUXrvm63Gd3++23h9yee+5ZiidOnFjXsVK5JlxvvfVWyC211FKl+Jhjjql5rFyjrq233jrkLrroolLc1RrX9O3btxTnmg7lGrTcfffdpXjEiBGhpqP/luuss07IzTvvvDVf9z//8z+lONcAh2paYIEFQm6JJZao+boPP/ww5EaOHNmSMZGXO9+vvfbapTj3eebuT9Zaa61S/L3vfS/UPP744yG30EILleJcw/Lc9TOdU//1X/8VanLHSs+B119/fai56aabvvY11C/XyDk3f9Km0MOHDw815513Xsil9zq5zzx3r5U2g8x9F3JNg9Prc+7+iPqkTd6KIjbDzM2V3r17h9yoUaNKcUd/Z3Pz/Isvvgg5z2Pttdpqq5XiwYMHh5p6mtvn7kduueWWkEsbE+bmXe790nv43H3xj3/845A75JBDSnE9DRZzYxo/fnzI5daoOpNfWAMAAAAAUAkWrAEAAAAAqAQL1gAAAAAAVII9rNsst1dNuudMUcR9Z95///1Qc+qpp5Zi+6i1X7qv0HLLLRdqjjvuuJCbb775SvHYsWNDzbnnnhtyf/vb30qxvTopiqIYOHBgyC2//PIh98EHH5TiG2+8MdQ4b3S+vfbaK+Ry+xBPmTKlFD/99NOhZo899gi5jz/+uBQ3+pnneilMmjQp5B5++OFSnPu3pLnctTE3z9O63JhmFbm/W7oHX64HxuTJk0PuT3/6Uynu6GtJ7vPN7Sub1uX2ebznnntK8aw8B2Z26Rz+4Q9/GGpy+3ume3necMMNoeazzz5rcnT8v9LPKrc/bLq3b+77mds3M+1RcNRRR4Wa3P6tuX2CU7nvfzrOueeeu+ZxiiJez4444ohQY5/ZxqVzLPfdzz0fPfTQQ6U4d++T+1zSPdZz15wNN9ww5NI5nLsW566huWd1GnP++eeHXHpOyn0uEyZMCLmO/s6m40r7CRWFZ692y82NHj16lOLc9St3Tko/q9x1aZtttgm5N998sxSnPRmml1t55ZVLca6fT26c6b85N8fS70LueSHXdyi3b3dn8gtrAAAAAAAqwYI1AAAAAACVYMEaAAAAAIBKsGANAAAAAEAlaLrYhNwG76m111475HJNaFLXXXddyI0fP76+gdEyaQOPu+66K9QsssgiIZdu7H/aaaeFmj/84Q8hlzZZ06Sha0rPLbnGemkziaIoiv/5n/8pxW+99VZrB0ZD0sZyK620UqjJNWlJzyMnnXRSqPnkk09CrqPPG2mDq1wTrlRujLn52tUbXi2xxBKlOL1GFEW+icrEiRPbNqac9JzVp0+fULP99tuHXPr55u5z0qaLrovVlX73991331CTu3dOz3UXX3xxawdGkH6Pcteg9J7iG9/4RqhZZZVVQq53796lOHdN6Nu3b8ilcyN3HnvnnXdCbvHFFy/FuaaLuWvJlVdeWYrThsU0J22GmWvGO++884Zc2mx4yJAhoeYnP/lJyPXv378U5xqV5e6d07mRG2euYVu9zT2J0r9x7jxSzzpL7jNOP5eObtibu0ep59/i3qZxub/dE088UYpPOOGEUHP00UeHXHpOmmeeeUJNbl0nPY/kxpSeE4siNhfNnX9y0vfLNYYdNWpUKT755JNDzc0331zz2J3NL6wBAAAAAKgEC9YAAAAAAFSCBWsAAAAAACrBgjUAAAAAAJWg6WKL9ezZsxSfcsopNWuKoig++OCDUnz66ae3dmDUlGuIcMUVV5TiRRddtK7XpY1b/vznP4eaXAMPKIrYgGH33Xev63W33XZbKZ46dWrLxkTj0iYbyy23XKjJNdlIG2i8+OKLoaajG2PkxrntttuW4nqaLuYafOWa2nb1JjSjR48uxbmmiwsuuGDIbbnllqX4pZdeCjUTJkwIufTvnWsQk2uomH7me+21V13jTN/vhRdeCDUffvhhyFFNaaPq+eefv67Xpc1jc/OV9srdL6TXnJ/97GehJndNmDRpUs33yzVLS8eQu0/ONcD6zW9+U4qHDh1a15jOPffcUtzVrzetlv4955prrlAzePDgkFtzzTVLca9evUJN7tqUPo/lmpDlzi3psdImnkWRb9y3wQYblOK333471OSu2US563w6f3LP27nn8mHDhpXi22+/PdTkzgfp8UeOHBlqnnrqqZBLr1+5dZ7cHE7/zWPHjg01uXtl6pN+Zy+88MJQc+utt4Zc2pgwdz3JPeek75c7R+WavtbTZDF3bUrn8Pnnnx9qLrjgglI8bty4UFO1Bos5fmENAAAAAEAlWLAGAAAAAKASLFgDAAAAAFAJ9rBuQm4/mRVWWKEUp/tbFUV+r5hLL720FKd7WtN+uX2w1l133Zqvy+37d8ABB5TidH8r+DoLLbRQKU73BS2Kovj0009D7tFHH23bmGhcuufmwIEDQ026b3lRxP0769kXtJVyY9p1111Dbt999635utS7774bcq+//nr9g5sF5e4p0v02X3755VCzySabhNz3v//9Uty/f/9Q88wzz4Rcej1bbbXVQs3CCy8ccukefGuvvXaoye3rmMrtD2kPx5nHpptuWopzezjm7oFvueWWUuwz73i58096DXrvvfda9n4TJ05s6HW5fYl79+5dinPzJ3d/lLsO0TrpnMp9drnrQrrvc25u5nLp637729+GmnPOOSfkll566VJ88MEHh5rc83x6/G9+85uhZv/99y/FM8N+sR0hvWfI9XoaNGhQKc7NldxewiuttFIp/sY3vlHz/Ysizqnc/uO5c0Z6vknPR9PLpff0++23X6hJ91i2z37jcp/nq6++GnJ77713Ke7Xr1+oyc2fdK1nrbXWCjV//etfQ65v375xsIncHvqHHXZYKb722mtDzayyh75fWAMAAAAAUAkWrAEAAAAAqAQL1gAAAAAAVIIFawAAAAAAKkHTxSbkmsmkG57PNddcoSa3wfuZZ55Zim2q3/GGDBkScvU0YPjXv/4VcnfffXfrBtbJunXrVooXWWSRUJNrxJM27zKn89K/b1EUxc4771yKc41Gcg3qxo0b17Jx0Tq560A90u9MrrlMPXJzLJdL59nhhx8eao466qiQm3vuuWseOz0fnH322aEm10i0q0sbMB9//PGh5pRTTgm5tInUZpttFmrWX3/9kEvnavrZFkVRjBo1quY400ajRZFvUpM2oMo15XLtqKZcc9VjjjmmFOc+87SRX1HExuM+c6Zniy22CLn0/j3XoPjUU08NOdec9kq/x+PHjw81aeOwoiiK5ZZbrhRvvPHGoWaFFVYIub///e+leNiwYaEmNzfS3B//+MdQs8Yaa4TcYostVor32GOPUHPTTTeV4ttuuy3UdEXptf+yyy4LNXPOOWcpzv19e/XqFXLp/Uc9Te2KIt675q5fCyywQMj16NGj5utyufTfl7u/vv3220vxrNJEr8rS55VGm/OOGTMm5HLPg/WsNZ100kkh94c//KEUz8rNqv3CGgAAAACASrBgDQAAAABAJViwBgAAAACgEixYAwAAAABQCZouNuGb3/xmyC2//PKlON24vSiK4oADDgi5Tz75pHUDo6Zc87Jvf/vbIZc2nJo4cWKoOf3000OuVRvf5xp75poRNdKgqJ4GEEVRFJtsskkpzjXquuSSS0IubQCYa7RE/jPee++9S3GuudVdd90VcrmGZXS+9LuWNmgpinyjwnnnnfdr46IoirFjx9Z8/9z8WXTRRUMubf47dOjQUFPP2HPno/TcmWuwQ5TeQzz11FOh5uCDDw65NddcsxTnzve5+5Mll1yyFI8ePTrUPPDAAyE333zzleJ11lkn1NTT+GjChAkhpwFfNeXOR8sss0wpzp3Xck2an3zyydYNjFlG7tp13nnnhVw6z3JN8x555JGQc25pndx3Pf375q45uUaMjz76aCl+7LHHQk3umpaOIfeslzb7K4qi+Oijj0rxHXfcEWrSa1xRFMXFF19cinPz9cgjjyzF99xzT6jpis0/088h/QyKoih+9atfleJzzz23rmMPGjSoFOeeUZdddtmQS+dP7pk/bU5eFHGe5b4LOWndiiuuGGr69OlTitMG11RH+jyfu+bk1lnS78Kzzz4banJzeFZuspjyC2sAAAAAACrBgjUAAAAAAJVgwRoAAAAAgEqwh3WdFltssZD785//HHLp/jUvvfRSqMnt/0jHyu1FtvHGG4fcXHPNVYpz+7fusssuITdp0qRS/PDDD4ea3J7O6b6yuT22nnvuuZD77LPPSnFuX+SNNtqoFG+44YahZtVVVw25AQMGlOLcnn+5feIWWmihUrzffvuFGopigQUWCLmVVlqp5utye/rl9uaj86X78ua+Q7k979LzzxVXXBFqfvCDH4Rcuo9i7hy1//77h1yvXr1qjqkeuXPbVlttVYrTcxb1ye1Z9/rrr4fcG2+80dDx031Bc++XO898/PHHpTi3H2U9+8W+8847NWuohi233DLkcvszpu67776QmzJlSkvGxKwlt6frwgsvHHLpHsDXXXddqOlK+312hPRaUc/5vdE9w3Ovy12H0nuf3D1M7nXp8XPno9y+6Gn/iNx8XXDBBUtxrn/IqFGjQq6ryX3G6Z7n6bP19Dz++OOlePDgwaFm6623DrnDDz+8FPfr1y/U9O/fP+TS70Kj9865Pd5XXnnlUvzggw+GmtycTseQPk8URX5NI9dHhCj3Gad/u969e9d1rPR56Iwzzgg1Xb3XnV9YAwAAAABQCRasAQAAAACoBAvWAAAAAABUggVrAAAAAAAqQdPF6ejZs2cpzjWJyW2m/vnnn5fi3XbbLdRo/NH50gYtRVEUc889d8ilDTzSuCiK4j//8z9DbosttijFuSYG6VwpitgssU+fPqEmN/a33367FOcahgwaNKgU5xolpo0jcnLjzr3urbfeqnksiuKAAw4IubQRRu6c8cwzz7RtTLRWPU1YN9tss5BLzwdrrbVWqHnooYdqvn/uvFXPdz0n1xgnPSd8+9vfDjW5JqG0Rq7hTj0NWHNNYxq9P0nnRb3NtdL3S69lVNcOO+xQsyY3D//0pz+FXKPN2Ji1pNeqQw89NNTkGnumTaty98m5851517hGGsu1+++djqmeBov1yj1XHXPMMaX4kEMOCTV9+/Ytxbnmd+m9XlHEBnz1zt+0rivO8fTfnGvynXtGXWSRRUpxrsFiPY2F6/2bp/PztddeCzWjR49u6Nip3PzV/LxxN9xwQ8jV02Qx11jzoIMOKsU333xzXa/rSvzCGgAAAACASrBgDQAAAABAJViwBgAAAACgEuxhXeT3hUr3JR44cGCoye0nc+6555bi5557rrnB0RaTJ08OudyeQXvttVcpzu37mtsfNt2PLN0TvSiK4osvvgi5dD+r3D7T6f7GRRH3387tcZUbZyq3n1U6puuuuy7UnHnmmSH30Ucf1Xy/riY3f773ve/VfF3uc3n99ddbMCI6QvodyvU2GDVqVMj16tWrFOeuVY30aWcAAAe8SURBVLnzQavk9n8cO3ZsyKV72Y4YMSLUdMV9FKuulZ9Juq9jbp/O3Pul+yqm+71TDblzz2qrrRZy9ewd+sgjj7RuYMxS0nvlXI+Y3H1xeq1cd911Q82//vWvkEv7L7hO1S+998jd36Z7ibfy75s7J6X3LLnnrEbHkHvdCy+88LVxUcS5mXsezO2L3Oi/xRyOcn+T3DNqOqdy55rcfuOp3L1zbg/pdB/tX/7yl6Emveeu9/NN63LvT30GDBgQcrlePancZ7XvvvuG3B/+8Iear+vq/MIaAAAAAIBKsGANAAAAAEAlWLAGAAAAAKASLFgDAAAAAFAJmi4WRTHvvPOG3KWXXlqKc80dPvjgg5A7+eSTS7GN06sp1zBz//33D7m77rqrFJ922mmhZr755gu5tOHCxIkTQ82zzz4bcmnTkkGDBoWaN998M+TSRh+5BosjR44sxePGjQs19913X8hNmDChFOcaf6SNVYqiKL788suQ6+pyjcjmmWeekEubY9x///2hJtc4lJnDu+++G3IbbLBByD366KOlONesJ3dtStXbmCedU7fddluo+cEPfhByGqwy//zzl+I+ffqEmtw1IW06NGbMmNYOjJbI3VPkzkfpteuVV14JNa5dTE/v3r1LcXpeKYp807N0Ln7/+98PNXfeeWfIvfPOO6U492yQXis91/0f6bNA2vy9KOJ5I3cNyDWoq0fuc2j0WI1K778WWmihUJM2bNt5551DTa6ZffrMljYIpTm5Jqx//OMfS/GPfvSjUJNrLppe0z7++ONQk2vIecQRR5Ti1157LdTkzkm0V/q9TteCiqK+5pvPPfdcyF1zzTUh55pSm19YAwAAAABQCRasAQAAAACoBAvWAAAAAABUggVrAAAAAAAqocs1Xcxtln/eeeeFXNpIL9fI4eqrrw65zz77rInR0ZlyzUCuv/76r43bLbepfz25XGPEdA43usl/7v01hahP7u/0+OOPh9ziiy9eik899dRQo0nDrCXXhDVtOLXVVluFmlwDn7Rx1dixY0PNPffcE3JpU6pcc7SObmpE9eTuo/r27VuK33777VCTmzsPP/xwKc418KXz5a77aYPFooj3Hs8880yo6dGjR8j53CmK/L1rPdJzUq9evULNGmusEXLDhg0rxbnGdul5K3ce64rXxfQeNPcdTu95W3nf2mjD6UblzoGrr756KR40aFCoSa+NuYZ8uWOnDStzjW9z52Dqk/vOnnjiiaU4d5+87bbbhtyoUaNK8fvvvx9qcs96adPXRs9/tFb6DL7sssvW9br0+7jjjjuGGs/ujfELawAAAAAAKsGCNQAAAAAAlWDBGgAAAACASuhye1hvuummIZfbAzTdD23SpEmh5oILLgg5e9PQSrk9jzt7v+jOfv+ZWW6P+1122SXkevbsWYrHjx8farrinoldTTpfbr755lCTy0E75e5zXn/99VJ81llnhZoFFlgg5O67775S/NFHHzU3ONoit7fv2WefHXIbbbRRKb7oootCTW5vfCiKuA9yrv/CUkstFXLp/MxdF0eMGBFyuWe7VG7Pfurb2zvdZzq373Sjz81VeN5+8sknS/G5554balZYYYVSnOvvkO6BXBTx/i/XZ4nWSv/G6R73RVEUw4cPD7n0HJGbm/Xm6Fi5/ePvv//+mjW5z+6DDz4oxa+99lqTo+PfXIUBAAAAAKgEC9YAAAAAAFSCBWsAAAAAACrBgjUAAAAAAJUwyzddnGOOOUrxYYcdFmrmnHPOmsfJNUR49913Gx8YQFEUn3zySV05gCrINZtJmyX+6U9/qutY9TTuovPlPvPf/va3IXfxxRfXfJ1GU0xP2nRx6623DjX77LNPyD3zzDOlONd0ccqUKSFXz1x0TmpcFb/ruSaa9XzGuYbz48aNK8V33313qHnkkUdK8ZZbbhlqcnMz16Sdzpeb07m5wcxh3nnnDbm0QXjuM8+dM0444YRS3MpGqbnGj11p3vmFNQAAAAAAlWDBGgAAAACASrBgDQAAAABAJViwBgAAAACgEmb5pouzz17+J66++uqhJrdp+RdffFGKTzzxxFDz+eefNzc4AICZXNqUppXNZph5aFBHK+Ua3h933HEhV8XmflRTK89R6VpBGhdFbEh85ZVXhpqu1DwNOku3bt1CLl0nLIqiuO2220rx9ttvH2peeOGFkLvxxhubGN3X6+rnCL+wBgAAAACgEixYAwAAAABQCRasAQAAAACohFl+D+t0P6llllkm1PTu3Tvk0j2nAAAA6Bz2q2Zmks7Xrr4XLXSW3LXj3XffDbndd9+9FOvN0fn8whoAAAAAgEqwYA0AAAAAQCVYsAYAAAAAoBIsWAMAAAAAUAkNN13U9IJmmD80w/yhGeYPjTJ3aIb5QzPMH5ph/tAM84dmmD80yi+sAQAAAACoBAvWAAAAAABUQrcZ+Xl+t27d3i+KYnT7hsNMbslp06YtOL3/aP7wNcwdmmH+0Azzh2aYPzTD/KEZ5g/NMH9ohvlDM752/vzbDC1YAwAAAABAu9gSBAAAAACASrBgDQAAAABAJViwBgAAAACgEixYAwAAAABQCRasAQAAAACoBAvWAAAAAABUggVrAAAAAAAqwYI1AAAAAACVYMEaAAAAAIBK+P8ACpL5ccwDGhwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c07bc54a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#comparing the original images with the reconstructed images\n",
    "\n",
    "in_imgs = test_images[:10]\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./autoencoder.ckpt\")\n",
    "    reconstructed = sess.run(decoded, feed_dict={\n",
    "        inputs: in_imgs\n",
    "    })\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20, 4))\n",
    "\n",
    "for images, row in zip([in_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
