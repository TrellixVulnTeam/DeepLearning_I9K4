{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.download_dataset()\n",
    "train_val_dataset, test_dataset = helper.load_images_labels()\n",
    "train_val_images, train_val_labels = helper.extract_images_labels(train_val_dataset)\n",
    "test_images, test_labels = helper.extract_images_labels(test_dataset)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_val_images, train_val_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntax:\n",
    "\n",
    "1. placeholder: \n",
    "    - tf.placeholder(dtype, shape=None, name=None)\n",
    "2. Variable: \n",
    "    - tf.Variable(initial_values, name=None)\n",
    "    - tf.get_variable(name, shape=None, dtype=None)\n",
    "3. initialize values: \n",
    "    - tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
    "    - tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
    "    - tf.zeros(shape, dtype=tf.float32, name=None)\n",
    "    - tf.ones(shape, dtype=tf.float32, name=None)  \n",
    "4. Mathematical Ops:\n",
    "    - tf.add()\n",
    "    - tf.matmul()\n",
    "    - tf.multiply()\n",
    "    - tf.log()\n",
    "5. Activation functions:\n",
    "    - tf.nn.relu(features, name=None)\n",
    "    - sigmoid\n",
    "    - tf.nn.softmax(logits, axis=None, name=None, dim=None)\n",
    "    - tf.nn.softmax_cross_entropy_with_logits_v2(_sentinel=None, labels=None, logits=None, dim=-1, name=None)\n",
    "6. Loss calculation:\n",
    "    - tf.reduce_sum(input_tensor,  axis=None, keepdims=None, name=None, reduction_indices=None)\n",
    "    - tf.reduce_mean(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None)\n",
    "7. Optimization functions:\n",
    "    - gradient descent\n",
    "    - Adam optimizer\n",
    "8. Session handling functions:\n",
    "    - tf.initialize_all_variables()\n",
    "    - tf.reset_default_graph()\n",
    "9. Saving model:\n",
    "    - tf.train.saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, num_inputs, num_outputs, num_hidden_layer1, num_hidden_layer2, learning_rate, epochs, batch_size):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.float32, shape=[None, num_inputs], name=\"inputs_placeholder\")\n",
    "        self.outputs = tf.placeholder(tf.float32, shape=[None, num_outputs], name=\"outputs_placeholder\")\n",
    "        \n",
    "        self.weights_input_to_hidden = self.getWeights([num_inputs, num_hidden_layer1], var_name=\"weights_input_to_hidden\")\n",
    "        self.weights_hidden_to_hidden = self.getWeights([num_hidden_layer1, num_hidden_layer2], var_name=\"weights_hidden_to_hidden\")\n",
    "        self.weights_hidden_to_output = self.getWeights([num_hidden_layer2, num_outputs], var_name=\"weights_hidden_to_ouput\")\n",
    "        \n",
    "        self.bias_hidden_layer1 = self.getBias([num_hidden_layer1], var_name=\"bias_hidden_layer1\")\n",
    "        self.bias_hidden_layer2 = self.getBias([num_hidden_layer2], var_name=\"bias_hidden_layer2\")\n",
    "        \n",
    "        saver = tf.train.Saver({\n",
    "            \"weights_input_to_hidden\" : self.weights_input_to_hidden,\n",
    "            \"weights_hidden_to_hidden\" : self.weights_hidden_to_hidden,\n",
    "            \"weights_hidden_to_ouput\" : self.weights_hidden_to_output,\n",
    "            \"bias_hidden_layer1\" : self.bias_hidden_layer1,\n",
    "            \"bias_hidden_layer2\" : self.bias_hidden_layer2,\n",
    "        })\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def getWeights(self, dim, var_name):\n",
    "        \n",
    "        return tf.Variable(tf.truncated_normal(shape=dim, mean=0.0, stddev=1.0, dtype=tf.float32), name=var_name)\n",
    "    \n",
    "    def getBias(self, dim, var_name):\n",
    "        \n",
    "        return tf.Variable(tf.ones(shape=dim, dtype=tf.float32), name=var_name)\n",
    "        \n",
    "    def train(self, train_features, train_targets, validation_features, validation_targets):\n",
    "        \n",
    "        #forward pass\n",
    "        hidden_layer1 = tf.add(tf.matmul(self.inputs, self.weights_input_to_hidden), self.bias_hidden_layer1)\n",
    "        hidden_layer1 = tf.nn.relu(hidden_layer1)\n",
    "        hidden_layer2 = tf.add(tf.matmul(hidden_layer1, self.weights_hidden_to_hidden), self.bias_hidden_layer2)\n",
    "        hidden_layer2 = tf.nn.relu(hidden_layer2)\n",
    "        logits = tf.matmul(hidden_layer2, self.weights_hidden_to_output)\n",
    "        \n",
    "        #loss function - softmax with cross entropy\n",
    "#         softmax = tf.nn.softmax(logits)\n",
    "#         softmax_log = tf.log(softmax)\n",
    "#         cross_entropy = tf.multiply(self.outputs, softmax_log)\n",
    "#         loss = -tf.reduce_sum(cross_entropy)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.outputs))\n",
    "        \n",
    "        #optimization function\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        \n",
    "        #accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(self.outputs, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        #initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        save_file = \"./model.ckpt\"\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            \n",
    "            num_records = len(train_features)\n",
    "            for epoch in range(self.epochs):\n",
    "                for ii in range(0, num_records, self.batch_size):\n",
    "                    batch_features = train_features[ii: ii + self.batch_size]\n",
    "                    batch_targets = train_targets[ii: ii + self.batch_size]\n",
    "                    feed_dict = {\n",
    "                        self.inputs : batch_features,\n",
    "                        self.outputs : batch_targets\n",
    "                    }\n",
    "                    sess.run(optimizer, feed_dict=feed_dict)\n",
    "                    \n",
    "                if epoch % 2 == 0:\n",
    "                    validation_accuracy = sess.run(accuracy, feed_dict={\n",
    "                        self.inputs : validation_features,\n",
    "                        self.outputs : validation_targets\n",
    "                    })\n",
    "                    print(\"Epoch {:<3} Loss {} Validation Accuracy {:0.3f}\".format(epoch, loss, validation_accuracy))\n",
    "            \n",
    "            saver.save(sess, save_file)\n",
    "            print(\"Model is trained and saved to disk\")\n",
    "            \n",
    "    def test(test_images, test_labels):\n",
    "        \n",
    "        save_file = \"./model.ckpt\"\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, save_file)\n",
    "            test_accuracy = sess.run(accuracy, feed_dict={\n",
    "                self.inputs : test_images, \n",
    "                self.labels : test_labels\n",
    "            })\n",
    "        print(\"Test Accuracy {0}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NeuralNetwork(\n",
    "    num_inputs = 3072,\n",
    "    num_outputs = 10,\n",
    "    num_hidden_layer1 = 256,\n",
    "    num_hidden_layer2 = 64,\n",
    "    learning_rate = 0.1,\n",
    "    epochs = 100,\n",
    "    batch_size = 128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   Loss Tensor(\"Mean:0\", shape=(), dtype=float32) Validation Accuracy 0.099\n",
      "Epoch 2   Loss Tensor(\"Mean:0\", shape=(), dtype=float32) Validation Accuracy 0.099\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-aed99e4a335c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-789b4932788d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_features, train_targets, validation_features, validation_targets)\u001b[0m\n\u001b[0;32m     74\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                     }\n\u001b[1;32m---> 76\u001b[1;33m                     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    952\u001b[0m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \"\"\"\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network.train(train_images, train_labels, val_images, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
