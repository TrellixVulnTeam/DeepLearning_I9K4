{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading MNIST datasets\n",
    "def download_dataset():\n",
    "    \n",
    "    from urllib.request import urlretrieve\n",
    "    from os.path import isfile, isdir\n",
    "    from tqdm import tqdm\n",
    "    import tarfile\n",
    "\n",
    "\n",
    "    cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "    class DLProgress(tqdm):\n",
    "        last_block = 0\n",
    "\n",
    "        def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "            self.total = total_size\n",
    "            self.update((block_num - self.last_block) * block_size)\n",
    "            self.last_block = block_num\n",
    "\n",
    "    if not isfile(tar_gz_path):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "            urlretrieve(\n",
    "                'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "                tar_gz_path,\n",
    "                pbar.hook)\n",
    "\n",
    "    if not isdir(cifar10_dataset_folder_path):\n",
    "        with tarfile.open(tar_gz_path) as tar:\n",
    "            tar.extractall()\n",
    "            tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the images dataset\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "def one_hot_encoding(x):\n",
    "    \n",
    "    n_values = 10\n",
    "    return np.eye(n_values)[x]\n",
    "\n",
    "def normalize(x):\n",
    "    \n",
    "    return x/255\n",
    "\n",
    "def load_images_labels():\n",
    "    \n",
    "    import pickle\n",
    "    from sklearn.utils import shuffle\n",
    "    \n",
    "    batch_ids = 5\n",
    "    train_validation_dataset = []\n",
    "    \n",
    "    for batch_id in range(1, batch_ids + 1):\n",
    "        with open(cifar10_dataset_folder_path + \"/data_batch_\" + str(batch_id), mode='rb' ) as file:\n",
    "            batch = pickle.load(file, encoding='latin1')\n",
    "            for data, label in zip(batch['data'], batch['labels']):\n",
    "                train_validation_dataset.append((normalize(data), one_hot_encoding(label)))\n",
    "    \n",
    "    test_dataset = []\n",
    "    with open(cifar10_dataset_folder_path + \"/test_batch\", mode='rb') as file:\n",
    "        for data, label in zip(batch['data'], batch['labels']):\n",
    "            test_dataset.append((normalize(data), one_hot_encoding(label)))\n",
    "            \n",
    "    return shuffle(shuffle(train_validation_dataset)), test_dataset\n",
    "\n",
    "def extract_images_labels(images_labels):\n",
    "    \n",
    "    images = np.array([x[0] for x in images_labels])\n",
    "    labels = np.array([x[1] for x in images_labels])\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset()\n",
    "train_val_dataset, test_dataset = load_images_labels()\n",
    "train_images, train_labels = extract_images_labels(train_val_dataset)\n",
    "test_images, test_labels = extract_images_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, num_inputs, num_hidden_layer1, num_hidden_layer2, num_output, learning_rate, epochs):\n",
    "        \n",
    "        self.inputs = tf.placeholder((None, num_inputs), name=\"inputs_placeholder\")\n",
    "        self.weights_input_to_hidden = tf.Variable((num_inputs, num_hidden_layer1), name=\"weights_input_to_hidden\")\n",
    "        self.weights_hidden_to_hidden = tf.Variable((num_hidden_layer1, num_hidden_layer2), name=\"weights_hidden_to_hidden\")\n",
    "        self.weights_hidden_to_output = tf.Variable((num_hidden_layer2, num_output), name=\"weights_hidden_to_ouput\")\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def train(self, features, targets):\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            #forward pass\n",
    "            self.forward_pass(features, targets)\n",
    "            #backpropagation\n",
    "            #update weights\n",
    "            #compute error\n",
    "    \n",
    "    def forward_pass(self, features, targets):\n",
    "        \n",
    "        \n",
    "    def test():\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
