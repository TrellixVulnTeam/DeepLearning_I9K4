{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.download_dataset()\n",
    "train_val_dataset, test_dataset = helper.load_images_labels()\n",
    "train_val_images, train_val_labels = helper.extract_images_labels(train_val_dataset)\n",
    "test_images, test_labels = helper.extract_images_labels(test_dataset)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_val_images, train_val_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(x):\n",
    "    return np.reshape(x, [-1, 32, 32, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_images = reshape(train_val_images)\n",
    "test_images = reshape(test_images)\n",
    "train_images = reshape(train_images)\n",
    "val_images = reshape(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    \n",
    "    def __init__(self, input_num, output_num, learning_rate, epochs, batch_size):\n",
    "        \n",
    "        height = input_num[0]\n",
    "        width = input_num[1]\n",
    "        depth = input_num[2]\n",
    "        \n",
    "        self.inputs = tf.placeholder(shape=[None, height, width, depth], dtype=tf.float32)\n",
    "        self.outputs = tf.placeholder(shape=[None, output_num], dtype=tf.float32)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        weights, bias = self.initialize_weights()\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        weights_conv1 = tf.Variable(tf.truncated_normal(shape=[2, 2, 3, 32]))\n",
    "        bias_conv1 = tf.Variable(tf.truncated_normal(shape=[32]))\n",
    "        \n",
    "        weights_conv2 = tf.Variable(tf.truncated_normal(shape=[2, 2, 32, 64]))\n",
    "        bias_conv2 = tf.Variable(tf.random_normal(shape=[64]))\n",
    "        \n",
    "        weights_conv3 = tf.Variable(tf.truncated_normal(shape=[2, 2, 64, 128]))\n",
    "        bias_conv3 = tf.Variable(tf.random_normal(shape=[128]))\n",
    "        \n",
    "        weights_fc1 = tf.Variable(tf.truncated_normal(shape=[4*4*128, 512]))\n",
    "        bias_fc1 = tf.Variable(tf.random_normal(shape=[512]))\n",
    "        \n",
    "        weights_fc2 = tf.Variable(tf.truncated_normal(shape=[512, 128]))\n",
    "        bias_fc2 = tf.Variable(tf.random_normal(shape=[128]))\n",
    "        \n",
    "        weights_output_layer = tf.Variable(tf.truncated_normal(shape=[128, 10]))\n",
    "        bias_output = tf.Variable(tf.random_normal(shape=[10]))\n",
    "        \n",
    "        weights = {\n",
    "            'conv1' : weights_conv1,\n",
    "            'conv2' : weights_conv2,\n",
    "            'conv3' : weights_conv3,\n",
    "            'fully1' : weights_fc1,\n",
    "            'fully2' : weights_fc2,\n",
    "            'output' : weights_output_layer\n",
    "        }\n",
    "        \n",
    "        bias = {\n",
    "            'conv1' : bias_conv1,\n",
    "            'conv2' : bias_conv2,\n",
    "            'conv3' : bias_conv3,\n",
    "            'fully1' : bias_fc1,\n",
    "            'fully2' : bias_fc2,\n",
    "            'output' : bias_output\n",
    "        }\n",
    "        \n",
    "        return weights, bias\n",
    "    \n",
    "    def construct_network(self):\n",
    "        \n",
    "        conv_layer1 = self.conv2D(self.inputs, self.weights['conv1'], self.bias['conv1'])\n",
    "        conv_layer1 = self.maxpool2D(conv_layer1)\n",
    "        \n",
    "        conv_layer2 = self.conv2D(conv_layer1, self.weights['conv2'], self.bias['conv2'])\n",
    "        conv_layer2 = self.maxpool2D(conv_layer2)\n",
    "        \n",
    "        conv_layer3 = self.conv2D(conv_layer2, self.weights['conv3'], self.bias['conv3'])\n",
    "        conv_layer3 = self.maxpool2D(conv_layer3)\n",
    "        \n",
    "        conv_layer = tf.reshape(conv_layer3, [-1, 4*4*128])\n",
    "        \n",
    "        \"\"\"\n",
    "        formula for calculating the new dimensions wrt convolution\n",
    "        When Padding is 'SAME'\n",
    "            new_height = ceil( float(input_height) / float(strides[0]) )\n",
    "            new_width = ceil( float(input_width) / float(strides[1]) )\n",
    "        When Padding is 'VALID'\n",
    "            new_height = ceil( float(input_height - filter_height + 1) / float(strides[0]) )\n",
    "            new_width = ceil( float(input_width - filter_width + 1) / float(strides[1]) )\n",
    "            \n",
    "        formula for calculating the new dimensions wrt max pooling\n",
    "            new_height = ( (input_height - filter_height) / stride[0] ) + 1\n",
    "            new_width = ( (input_width - filter_width) / stride[0] ) + 1\n",
    "        \"\"\"\n",
    "        \n",
    "        fully_connected_layer1 = tf.add(tf.matmul(conv_layer, self.weights['fully1']), self.bias['fully1'])\n",
    "        fully_connected_layer1 = tf.nn.relu(fully_connected_layer1)\n",
    "        fully_connected_layer1 = tf.nn.dropout(fully_connected_layer1, self.keep_prob)\n",
    "        \n",
    "        fully_connected_layer2 = tf.add(tf.matmul(fully_connected_layer1, self.weights['fully2']), self.bias['fully2'])\n",
    "        fully_connected_layer2 = tf.nn.relu(fully_connected_layer2)\n",
    "        fully_connected_layer2 = tf.nn.dropout(fully_connected_layer2, self.keep_prob)\n",
    "        \n",
    "        output_layer = tf.add(tf.matmul(fully_connected_layer2, self.weights['output']), self.bias['output'])\n",
    "        logits = tf.nn.softmax(output_layer)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def conv2D(self, inputs, weights, bias, strides=1, padding='SAME'):\n",
    "        conv = tf.nn.conv2d(inputs, weights, strides=[1, strides, strides, 1], padding=padding)\n",
    "        conv = tf.nn.bias_add(conv, bias)\n",
    "        return tf.nn.relu(conv)\n",
    "    \n",
    "    def maxpool2D(self, inputs, k=2, strides=2, padding='SAME'):\n",
    "        # ksize refers to the filter\n",
    "        # strides refer to the num of steps the filter should move\n",
    "        return tf.nn.max_pool(inputs, ksize=[1, k, k, 1], strides=[1, strides, strides, 1], padding=padding)\n",
    "    \n",
    "    def calculate_cost(self, logits):\n",
    "        \n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.outputs))\n",
    "    \n",
    "    def calculate_optimizer(self, loss):\n",
    "        \n",
    "        return tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "    \n",
    "    def calculate_accuracy(self, logits):\n",
    "        \n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(self.outputs, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        return accuracy\n",
    "        \n",
    "    def train(self, train_images, train_labels, validation_images, validation_labels, dropout=0.2):\n",
    "        \n",
    "        # num_records = len(train_images)\n",
    "        num_records = train_images.shape[0]\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "#             weights, bias = self.initialize_weights()\n",
    "            \n",
    "            for epoch in range(self.epochs):\n",
    "                for ii in range(0, num_records, self.batch_size):\n",
    "\n",
    "                    batch_train_images = train_images[ii: ii + self.batch_size]\n",
    "                    batch_train_labels = train_labels[ii: ii + self.batch_size]\n",
    "\n",
    "                    # Neural network\n",
    "                    logits = self.construct_network()\n",
    "                    # cost function\n",
    "                    cost = self.calculate_cost(logits)\n",
    "                    # optimization function\n",
    "                    optimizer = self.calculate_optimizer(cost)\n",
    "                    # calculate accuracy\n",
    "                    accuracy = self.calculate_accuracy(logits)\n",
    "\n",
    "                    sess.run(optimizer, feed_dict={\n",
    "                        self.inputs : batch_train_images,\n",
    "                        self.outputs : batch_train_labels,\n",
    "                        self.keep_prob : 0.2\n",
    "                    })\n",
    "\n",
    "                    loss = sess.run(cost, feed_dict={\n",
    "                        self.inputs : batch_train_images,\n",
    "                        self.outputs : batch_train_labels,\n",
    "                        self.keep_prob : 1.0\n",
    "                    })\n",
    "\n",
    "                    validation_accuracy = sess.run(accuracy, feed_dict={\n",
    "                        self.inputs : validation_images, \n",
    "                        self.outputs : validation_labels,\n",
    "                        self.keep_prob : 1.0\n",
    "                    })\n",
    "\n",
    "                    print(\"epoch {0:<3} Loss {1:0.3f} Accuracy {2:0.3f}\".format(epoch, loss, validation_accuracy))\n",
    "                        \n",
    "                        \n",
    "                \n",
    "                \n",
    "    \n",
    "    def test(): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = CNN(\n",
    "    input_num = [32, 32, 3], \n",
    "    output_num = 10, \n",
    "    learning_rate = 0.1, \n",
    "    epochs = 100,\n",
    "    batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0.000000 Loss 2.375 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.367 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.321 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.367 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.352 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.328 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.414 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.336 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.453 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.344 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.344 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.305 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.367 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.367 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.336 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.321 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.383 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.360 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.352 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.328 Accuracy 0.101\n",
      "epoch 0.000000 Loss 2.336 Accuracy 0.101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-69017bdca017>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-771dbc089277>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_images, train_labels, validation_images, validation_labels, dropout)\u001b[0m\n\u001b[0;32m    166\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mvalidation_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m                     })\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network.train(train_images, train_labels, val_images, val_labels, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16929238423097681040\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
