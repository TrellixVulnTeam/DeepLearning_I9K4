{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsund\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.download_dataset()\n",
    "train_val_dataset, test_dataset = helper.load_images_labels()\n",
    "train_val_images, train_val_labels = helper.extract_images_labels(train_val_dataset)\n",
    "test_images, test_labels = helper.extract_images_labels(test_dataset)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_val_images, train_val_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(x):\n",
    "    return np.reshape(x, [-1, 32, 32, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_images = reshape(train_val_images)\n",
    "test_images = reshape(test_images)\n",
    "train_images = reshape(train_images)\n",
    "val_images = reshape(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    \n",
    "    def __init__(self, input_num, output_num, learning_rate, epochs, batch_size):\n",
    "        \n",
    "        height = input_num[0]\n",
    "        width = input_num[1]\n",
    "        depth = input_num[2]\n",
    "        \n",
    "        self.inputs = tf.placeholder(shape=[None, height, width, depth], dtype=tf.float32)\n",
    "        self.outputs = tf.placeholder(shape=[None, output_num], dtype=tf.float32)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        weights, bias = self.initialize_weights()\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        weights_conv1 = tf.Variable(tf.truncated_normal(shape=[2, 2, 3, 32], mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "        bias_conv1 = tf.Variable(tf.zeros(shape=[32]))\n",
    "        \n",
    "        weights_conv2 = tf.Variable(tf.truncated_normal(shape=[2, 2, 32, 64], mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "        bias_conv2 = tf.Variable(tf.zeros(shape=[64]))\n",
    "        \n",
    "        weights_conv3 = tf.Variable(tf.truncated_normal(shape=[2, 2, 64, 128], mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "        bias_conv3 = tf.Variable(tf.zeros(shape=[128]))\n",
    "        \n",
    "        weights_fc1 = tf.Variable(tf.truncated_normal(shape=[4*4*128, 512], mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "        bias_fc1 = tf.Variable(tf.zeros(shape=[512]))\n",
    "        \n",
    "        weights_fc2 = tf.Variable(tf.truncated_normal(shape=[512, 128], mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "        bias_fc2 = tf.Variable(tf.zeros(shape=[128]))\n",
    "        \n",
    "        weights_output_layer = tf.Variable(tf.truncated_normal(shape=[128, 10], mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "        bias_output = tf.Variable(tf.zeros(shape=[10]))\n",
    "        \n",
    "        weights = {\n",
    "            'conv1' : weights_conv1,\n",
    "            'conv2' : weights_conv2,\n",
    "            'conv3' : weights_conv3,\n",
    "            'fully1' : weights_fc1,\n",
    "            'fully2' : weights_fc2,\n",
    "            'output' : weights_output_layer\n",
    "        }\n",
    "        \n",
    "        bias = {\n",
    "            'conv1' : bias_conv1,\n",
    "            'conv2' : bias_conv2,\n",
    "            'conv3' : bias_conv3,\n",
    "            'fully1' : bias_fc1,\n",
    "            'fully2' : bias_fc2,\n",
    "            'output' : bias_output\n",
    "        }\n",
    "        \n",
    "        return weights, bias\n",
    "    \n",
    "    def construct_network(self):\n",
    "        \n",
    "        conv_layer1 = self.conv2D(self.inputs, self.weights['conv1'], self.bias['conv1'])\n",
    "        conv_layer1 = self.maxpool2D(conv_layer1)\n",
    "        \n",
    "        conv_layer2 = self.conv2D(conv_layer1, self.weights['conv2'], self.bias['conv2'])\n",
    "        conv_layer2 = self.maxpool2D(conv_layer2)\n",
    "        \n",
    "        conv_layer3 = self.conv2D(conv_layer2, self.weights['conv3'], self.bias['conv3'])\n",
    "        conv_layer3 = self.maxpool2D(conv_layer3)\n",
    "        \n",
    "        conv_layer = tf.reshape(conv_layer3, [-1, 4*4*128])\n",
    "        \n",
    "        \"\"\"\n",
    "        formula for calculating the new dimensions wrt convolution\n",
    "        When Padding is 'SAME'\n",
    "            new_height = ceil( float(input_height) / float(strides[0]) )\n",
    "            new_width = ceil( float(input_width) / float(strides[1]) )\n",
    "        When Padding is 'VALID'\n",
    "            new_height = ceil( float(input_height - filter_height + 1) / float(strides[0]) )\n",
    "            new_width = ceil( float(input_width - filter_width + 1) / float(strides[1]) )\n",
    "            \n",
    "        formula for calculating the new dimensions wrt max pooling\n",
    "            new_height = ( (input_height - filter_height) / stride[0] ) + 1\n",
    "            new_width = ( (input_width - filter_width) / stride[0] ) + 1\n",
    "        \"\"\"\n",
    "        \n",
    "        fully_connected_layer1 = tf.add(tf.matmul(conv_layer, self.weights['fully1']), self.bias['fully1'])\n",
    "        fully_connected_layer1 = tf.nn.relu(fully_connected_layer1)\n",
    "        fully_connected_layer1 = tf.nn.dropout(fully_connected_layer1, self.keep_prob)\n",
    "        \n",
    "        fully_connected_layer2 = tf.add(tf.matmul(fully_connected_layer1, self.weights['fully2']), self.bias['fully2'])\n",
    "        fully_connected_layer2 = tf.nn.relu(fully_connected_layer2)\n",
    "        fully_connected_layer2 = tf.nn.dropout(fully_connected_layer2, self.keep_prob)\n",
    "        \n",
    "        output_layer = tf.add(tf.matmul(fully_connected_layer2, self.weights['output']), self.bias['output'])\n",
    "        #logits = tf.nn.softmax(output_layer)\n",
    "        \n",
    "        return output_layer\n",
    "    \n",
    "    def conv2D(self, inputs, weights, bias, conv_strides=1, conv_padding='SAME'):\n",
    "        conv = tf.nn.conv2d(inputs, weights, strides=[1, conv_strides, conv_strides, 1], padding=conv_padding)\n",
    "        conv = tf.nn.bias_add(conv, bias)\n",
    "        return tf.nn.relu(conv)\n",
    "    \n",
    "    def maxpool2D(self, inputs, k=2, pool_strides=2, pool_padding='SAME'):\n",
    "        # ksize refers to the filter\n",
    "        # strides refer to the num of steps the filter should move\n",
    "        return tf.nn.max_pool(inputs, ksize=[1, k, k, 1], strides=[1, pool_strides, pool_strides, 1], padding=pool_padding)\n",
    "    \n",
    "    def calculate_cost(self, outputs):\n",
    "        \n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=self.outputs)\n",
    "        cost = tf.reduce_mean(loss)\n",
    "        return loss, cost\n",
    "    \n",
    "    def calculate_optimizer(self, loss):\n",
    "        \n",
    "        #return tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        return tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "    \n",
    "    def calculate_accuracy(self, outputs):\n",
    "        \n",
    "        logits = tf.nn.softmax(outputs)\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(self.outputs, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        return accuracy\n",
    "        \n",
    "    def train(self, train_images, train_labels, validation_images, validation_labels, dropout=0.2):\n",
    "        \n",
    "        num_records = train_images.shape[0]\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            sess.run(init)\n",
    "            \n",
    "            for epoch in range(self.epochs):\n",
    "                for ii in range(0, num_records, self.batch_size):\n",
    "\n",
    "                    batch_train_images = train_images[ii: ii + self.batch_size]\n",
    "                    batch_train_labels = train_labels[ii: ii + self.batch_size]\n",
    "\n",
    "                    # Neural network\n",
    "                    outputs = self.construct_network()\n",
    "                    # cost function\n",
    "                    loss, cost = self.calculate_cost(outputs)\n",
    "                    # optimization function\n",
    "                    optimizer = self.calculate_optimizer(loss)\n",
    "                    # calculate accuracy\n",
    "                    accuracy = self.calculate_accuracy(outputs)\n",
    "\n",
    "                    sess.run(optimizer, feed_dict={\n",
    "                        self.inputs : batch_train_images,\n",
    "                        self.outputs : batch_train_labels,\n",
    "                        self.keep_prob : 0.5\n",
    "                    })\n",
    "\n",
    "                    loss = sess.run(cost, feed_dict={\n",
    "                        self.inputs : batch_train_images,\n",
    "                        self.outputs : batch_train_labels,\n",
    "                        self.keep_prob : 1.0\n",
    "                    })\n",
    "\n",
    "                    validation_accuracy = sess.run(accuracy, feed_dict={\n",
    "                        self.inputs : validation_images, \n",
    "                        self.outputs : validation_labels,\n",
    "                        self.keep_prob : 1.0\n",
    "                    })\n",
    "\n",
    "                    print(\"epoch {0:<3} Loss {1:0.3f} Accuracy {2:0.3f}\".format(epoch, loss, validation_accuracy))\n",
    "                        \n",
    "\n",
    "    def test(): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = CNN(\n",
    "    input_num = [32, 32, 3], \n",
    "    output_num = 10, \n",
    "    learning_rate = 0.001, \n",
    "    epochs = 100,\n",
    "    batch_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network.train(train_images, train_labels, val_images, val_labels, dropout=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [21.0, 43.0, 55.0, 32.0, 88.0]\n",
    "y = tf.placeholder(shape=(5,), dtype=tf.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.nn.softmax(y)\n",
    "p = tf.nn.softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1488 0.1488 0.1488 0.1488 0.4045]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    xy = sess.run(p, feed_dict={\n",
    "        y: x\n",
    "    })\n",
    "    print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
